Hello to all of you. My name is Dr. Himantra Athar and we are in the course Machine Learning. We are in the module 3, Introduction to Machine Learning. And in this video, video number 3, we are going to look into evaluating model performance and common issues.
So, the learning objective of this video is we are going to look into how to evaluate the performance of a classification model, what are the different performance matrices and we will look into the concepts of underfitting and overfitting in Machine Learning.
So, let's start right away. So, in the last video we saw that once we are building a classification model, it is divided into phases.
First, we correct the data. Second, we split the data into two parts, train data and test data. On training data, we build the model. Once the model is built using any of the classification algorithm, then we are going to measure, we are going to test the model on the testing set.
In testing set, once the model is evaluated and its performance is good enough, then we are going to deploy that in real world. Right.
So, here, now we will talk about once we have built the model F, how do we measure the performance or evaluate the performance of the model F using various performance measures.
So, performance on model evaluation is a process of using various performance matrices to understand the model performance and its limitation. This is what we are going to discuss now. Right. So, right.
Once we have built the model F, right. It can be a decision tree model or a neural network model or a naive base model or a SVA model. Now, I want to check whether the model is good or bad. And for that, what we want to do is to measure its performance on the test set. Once that is done, if there are any issues, we are going to diagnose those issues and then fix them before deployment. So, how do we measure the performance of this classification model?
classification model right there are various performance matrices like accuracy, speed, robustness, scalability, interpretability and so on. There are few more as well. So let's start to discuss each of these performance matrices or evaluation matrices for classification model now. Right. So first one is called accuracy. Right. So what do you mean by accuracy?
Assuming as I said earlier as well I have training data like this. So this is my x and this is my y. Right. So a1, a2, a3 are x and y is my class label. And here I have tuples. Tuple 1, tuple 2, tuple 3 and so on. Right. So first I have been given this data set D.
What I have to do? First step is to break this into two parts, two disjoint sets. This is called train part and this is called test part. Test data and train data. Typically the ratio between test data and train data you will see in various articles as 70-30. So original data 70% is used for training the data. 30% is used for testing the data.
So original data. But there is no thumb rule. There is no thumb rule. There is no hard and fast rule. In some literature you will find the ratio as 80-20. In some you will find the ratio as 90-10 as well. In some literature you will find it as 60-40 as well.
So more or less majority of the time you will find the ratio as 70-30. So 70% is used for training the data and 30% is used for testing the data. So original data now split into two disjoint sets. One is called train data. Another is called test data. 70% of the data goes to training data and 30% of the data is going to test data.
Now I build the model F on the train data. I am going to measure the performance of this F on test data. This is what we want to achieve now. How do we do that? Using various performance matrices. So let's start with classification.
how do I measure the performance of a classification system let's see that so in classification system in training phase right we have used train data which consists of both x and y to train the model f right so I build this model f right
and which is essentially forming a relationship between x and y during testing phase again I have bought test data test data also consists of x and y x which is attributes and the corresponding class label y I will forget about this y for a while just forget about it right so in
this testing phase what we are going to do we are going to the model f which I have trained earlier right will be passed this x and it will be f will be asked to do prediction let's suppose my f is predicting for a particular x y dash so now how I am going to measure the performance of the system if y dash is equal to y that means the original class label
and the predicted class label so this is my predicted class label and this is my original class label so if the predicted class label and the original class label is matching then it is correct prediction so let's call it as cp right I'm just putting a counter else
so if y dash is not matching y so if y dash is not matching y right so else if let me write if y dash is not matching y then incorrect prediction right this is how I am essentially measuring the performance of the system so assuming like this now let's further break it down data set let's outpost my data set
data set had 1000 tuples I am doing split of train and test as let's suppose 90 10 right 90 percent goes to training and 10 percent goes for testing I am just taking a simple example and I am taking this ratio so that the calculations are easy you are able to understand you can take 60 30 60 40 70 30 80 20 your call right so 90 10 ratio I have divided
so 900 samples so 900 samples 900 tuples will be used for training and 100 tuples will be used for testing let's assume that this is a binary classification problem there are two classes y value is two classes y value is the number of unique values of y is two there are two unique classes malware or benign assuming I am building a malware
detection system antivirus a file can be used for testing and a malware detection system antivirus a file can be malware or a file can be benign malware means it's a virus benign means file is good right so all this training example tuples will have class labels some are m some are b and so on test also will have some are m some are b some are m some are b and so on this is how your original data would look like you have split the original data into train and test
train in this case is 90 percent test is 10 percent right which is 100 tuples now i am evaluating the performance of this 100 tuples against these 100 test tuples so this loop right so this would be a loop each test tuple will be passed into this f right so each test tuple will be passed in this f so f of test tuple 1 and will be asked to do prediction right so this would be a loop
loop running in this case 100 times you are going to test your model against 100 test tuples now you will see understand the meaning of counter if this counter is 100 after running the loop that means all the predictions were correct if correct prediction counter right cp++ is 100 that means all the predictions were correct if incorrect prediction was 100
assuming incorrect prediction value is 100 and cp prediction value is 0 so if this is 0 and this is 100 all the predictions were wrong right in this case all the predictions were wrong right in this case all the predictions were wrong right let's take another case if this is 50 correct prediction is 50 and this counter value is 50 then the your model is predicting half of the time it is correct half of the time it is wrong right so this is how we are measuring the performance of this classification system original data
split into two disjoint sets train data test data build the model f on training data once the model
is built right then you test that model f on test data right and then essentially you are in
classification system you have a counter to check how many predictions you are doing correct or not
assuming i have a regression system
again regression is also a supervised learning here also i am doing prediction here i am predicting
not a class label a continuous value so how i am going to evaluate the performance of a regression
model regression model how do i evaluate it very simple right so again the original data
right split into train and test some ratio do not worry about it right so
in training phase you have trained data you have built a model f right a regression model f
where if you pass x it will predict y a continuous value y would be a continuous value during testing
phase how do i evaluate this model f so in regression a formula will change very slightly how you you are
going to evaluate right so these samples will be passed to this function f right so when i pass this test
sample x right it is going to predict y let's suppose y dash so how do i measure the performance of the
system here the performance was a just a equation it was just a condition if y dash is y is equal to y
correct prediction if y dash is not equal to y it is incorrect prediction how you are going to measure if right or simple next step would be make y dash minus y
so if y dash minus y is equal to zero correct prediction
if else if y dash minus y is not equal to zero then incorrect prediction think about it let me explain
it with petrol price example petrol price prediction so here it was petrol price was 100 on a particular day
when you passed it to a train model regression model now this model predicted petrol price to be let's suppose y dash was
predicted as 101 so what you are doing in this to check the performance of this model 101 minus the original value 100 that means one so this was incorrect prediction right
if my model would have predicted as 100 then it was correct prediction right so essentially this is how
you measure the performance of a regression system let's expand it further right so you can calculate
something called absolute error so let's suppose these are 100 test tuples given to me absolute error would be y dash minus y
cause summation over 100 test tuples so absolute error of this regression model is y dash minus y for each test tuple and you do
it for all the test tuple this is summation function right so this is called absolute error there is something called average
absolute error right so what it would be summation of y dash minus y
over 100 samples over 100 samples in this case it is 100 test tuples but summation over all the test samples
1 by n where n is 100 so what is the average error in each prediction right so n is 100 in this case right
so n is number of test samples so what is the average error in each test tuple prediction so this is called average
absolute error there is some absolute error there is something called sum of square error what do you mean by sum of square error
very simple right so if i square this this is called sum of square error y dash minus y a square divided by 1 by n this is called sum of square error
right so this is how you measure the performance of regression models
but now let's come back to classification and further try to understand the performance matrices
so basically i explained you how to evaluate the performance of a classification model
a regression model let's come back to classification and further how do we evaluate it right so
now you understood this counter correct prediction incorrect prediction right so correct prediction is when
i am predicting it correct malware predicted as malware and incorrect predict is that the cross table was
wrongly predicted so correct prediction and incorrect prediction can be broken into two parts
correct prediction counter essentially is measuring two things when malware is predicted as malware and
benign is predicted as benign so in test tuple right if the original level was benign my model predicted is benign so that was
measured in correct prediction right when malware was predicted as malware again it was measured in correct prediction so your
correct prediction consists of two things malware is benign is benign is benign incorrect prediction is also containing of two things
when malware is predicted as benign and when benign is predicted as malware right these are two incorrect predictions so correct prediction is two parts
incorrect prediction is also two parts now further understand them better so when you look into classification problem
assuming i am assuming i am talking about two class classification problem two class
or binary classification two class means binary classification
assuming let's take the case of antivirus there are two classes
malware class a file is a malware or a file is benign file is good file is bad so there is
something called confusion matrix which measures all the performance matrices of classification problem so classification problem assuming it is a binary classification problem and there are two classes malware and benign so i can build something called confusion matrix right so there are four basically things we have to measure
so that is what we have to measure mm bb mb and bm all four we have to measure so that is what we are
measuring in confusion matrix so let me build a two cross two matrix and then put it here
right so you have y axis so this is y axis and this is my x axis on x axis i am writing predicted class
and on y axis i am writing true class original class so on x axis i am writing predicted class
so what are the predicted class that are possible malware benign what are the true classes that are
possible malware or benign these are the four combinations possible let me rough few of the items
okay so a file malware predicted class can be malware or benign true class can be malware or benign
let's assume malware is called as positive class and benign is called as negative class
right again malware is called as positive class and benign is called as negative class
right so in confusion matrix there is a term called true positive right so here my malware class is
positive class my benign class is negative class just the name change so true positive means when you
predict a positive class for a test tuple you predict a positive class you predict a malware
and the prediction is true so when malware is predicted as malware then it is called true positive this case all four cases i am going to measure
so when malware is predicted as malware this is called correct prediction and you are measuring the name of that performance metric is called true positive
then there is something called true negative so here negative class is benign class just the name change negative class is benign class so when you predict negative class is benign class so when you predict negative class and the prediction is true then it is called true negative so another way of saying is when you predict benign as benign that is called true negative both true positive and true negative are correct predictions
so when you predict when you predict when you predict when you predict when you predict when you detect benign as benign as benign then it is called true negative both are correct predictions now both correct predictions are done correct prediction part is done now let's talk about incorrect prediction so sometimes right you are predicting negative class but the original class is positive then it is called false positive then it is called false positive
false negative so you predicted negative so you predicted negative so you predicted negative class and the prediction is false this is called false negative so another way of saying this is that you predicted negative class you predicted the file as benign malware predicted as benign so this is prediction this is prediction this is prediction this is prediction so when malware is predicted as benign this is called false negative and another thing can be
you predicted positive you predicted positive you predicted positive but the prediction is false false positive so when we predicted benign file as malware then it is called false positive then it is called false positive then benign is predicted as malware false positive then malware is predicted as benign this is called false negative both of these fp and fn are wrong prediction to the circle are predicted class
and fp and fp and fp and fp and fp and fp and fp are original are original are original right so benign benign malware malware malware malware yeah got it so malware predicted as malware true positive benign predicted as benign true negative benign predicted as benign true negative benign predicted as malware false positive and malware predicted as benign false positive and malware predicted as benign false negative so these are four essential measures to
to measure the performance of any classification system these are very important and these are actually measured in something called confusion matrix so in a binary classification problem confusion matrix consists of four tuples four matrices true positive true negative false positive false negative i hope this is clear so how do i measure the overall performance of the system then you have something called accuracy what do you mean by accuracy accuracy is correct
prediction by accuracy is correct prediction divided by total prediction so how do you measure the correct prediction here if i am giving you tn tp fn fp how do you know correct prediction correct predictions are tp plus tn these are correct predictions
the correct predictions divided by total predictions divided by total predictions tp plus tn plus fp sorry fp plus fn so accuracy the formula to calculate accuracy is true positive plus true negative divided by true positive plus true negative plus false positive plus false negative sometimes people multiply it by 100 to get the accuracy
in terms of accuracy in terms of accuracy so this is what we measure what do you mean by accuracy now there are other measures as well to measure the performance of a classification system the fundamental performance matrices are always true positive true positive true negative false positive false negative and these four but from these i can derive something called accuracy correct predictions divided by total number of predictions now there are some additional performance measures to further understand
this confusion matrix better let's understand in terms of antivirus only right so there is something called precision of the model also known as ppv positive predictive value ppv positive predicted value and let me write the formula here first true positive divided by true positive plus false positive right now
let me try to explain what do we measure by precision right so when we are talking about precision look from this angle look from this angle look from this angle so it is true positive divided by true positive plus false positive so what we are measuring in precision we are saying that out of all the positive predictions i did my model did out of all the positive predictions my model did how many of them were correct
so this is my predicted so this is my predicted class so this is my predicted class these are all my positive predictions malware predictions when i predicted malware all these are when i predicted benign negative class so out of all the malware predictions i did malware malware malware malware malware out of all the test samples i predicted malware many times how many of them were correct so some of them are correct some of them were wrong so these were correct
so out of all the positive predictions i did how many of them were correct so this is called precision there is also something called negative predictive value npv so npv is this out of all the benign predictions i did all the times i said benign my model said benign the file is good how many of them were correct so correct benign predictions divided
by total benign predictions true negative divided by tn plus fn so precision and npv precision ppv and npv these are the two performance matrices i can derive from confusion matrix right so four fundamental performance matrices true positive true negative false positive false positive false negative from this i can derive accuracy which gives about overall summary i can derive ppv which talks about positive
positive predictive value out of all these i can derive npv which talks about negative predictive value so these are few more now let's understand a further more performance matrices so i looked here i looked here from left hand side let me look from top side now
so there is something called recall again a performance matrix very famous precision recall are extremely famous performance matrices right so what do you mean by recall when you look from top from true class perspective so recall formula is true positive divided by true positive plus false negative right so true positive divided by true positive plus false negative look from top so what we are saying is out of all
the malware samples in the training dataset, out of all the malware samples, positive class samples in the test dataset, how many of them you were able to detect properly? Some you were able to detect properly, some you were not able to detect. So some malware were detected as malware, but some malware were misclassified as benign, were wrongly predicted as benign. So the formula is correct detection of malware samples,
divided by all malware detections. So here you can see, TP divided by TP plus Fn. So out of all the malware samples in my detector dataset, how many of them I am able to detect, that is called recall, also known as TPR, true positive rate. TPR, true positive rate. And then there is something called true negative rate.
when you look at the negative class out of all the negative class elements all the tuples how many of them I am able to detect correctly so it will be true negative divided by two negative plus false positive right so out of all the benign tuples I had all the benign tuples I had how many of them I am able to detect properly this is called true negative rate and this is recall is called true positive rate
recall is also called as sensitivity and true negative rate is also called as selectivity so I hope you are clear with this right binary classification problem basically when you build confusion matrix you get true positive true negative false positive false negative from this four performance matrix I can derive additional performance matrices like accuracy precision
recall TPR FNR and so on right now let's come back right we were discussing about measuring the performance of the classification system and one way to measure the performance of the classification system is if it is doing some prediction how many predictions were correct now how many predictions when we talk about it can be true positive true negative false positive false negative and then you derive additional performance from that
that all will come under accuracy. So when you are doing prediction how the predictions were correct or incorrect. All those things will be measured using accuracy. There are other performance matrices in classification problem. Classification model. Another one is called speed. Speed of prediction. Right. So or speed of training. Right. So model speed means or computational speed means essentially what is your training time. How much time you took to
train the model F. To tune the model F. This is called training time. Right. So you can measure it in seconds or milliseconds and so on. Sometimes in deep neural networks it might take days or months to train this model F. Sometimes it takes months. Right. So training time is essentially very important. You can measure it in terms of days, minutes, seconds depending on your use case. And there is something called prediction time. Right. Or classification.
classification time. So when you deploy these models in real world how quickly you can do prediction. So I have trained this model F. Right. I pass it one real world sample or one test sample. One test sample. Test sample. How quickly it is doing the prediction. I can measure it using some time parameter. Right. So millisecond or so on. Right. So speed when we are talking about how quickly you can train a model. How quickly you can train a model. How quickly
quickly you can predict using that model. Both are really important and both are you are trying to minimize, you are trying to reduce. So your model might give you accuracy of 99% but it takes lot of time to training and lot of time for prediction when it is of no use. Think about it. Intrusion detection system. Right. Millions of packets coming inside your network. Millions of packets are going outside your network. Now if you take one hour to do one thing.
If you take one hour to do one thing, then your intrusion detection system is of no use. If your training time of intrusion detection system is months, then it is of no use. Right. So in some cases training time even higher is fine. Right. But prediction time is extremely important when you are predicting really fast. Another performance metric to measure a performance of the classification system is called interpretability. Right. So I have done a prediction. I predicted a test sample as malware or benign.
the question is interpretability means the question is how did my model predicted a particular test sample as malware or benign how what was the reason behind it if I can understand the decision making process this is called high interpretability or wide box scenario wide box prediction that means my classification model is predicting some class label right I have to measure right whether I can quantify or understand the decision making
process if I can understand the decision making process then it is called high interpretability or wide box scenario if I cannot understand the decision making process that happens in complex neural networks that often happens in complex neural networks I do not the model is too complex and I do not understand the hidden logic of prediction right then it is called low interpretability or black box classification model in an ideal world your model classification
model should have high interpretability that means whatever prediction it is doing correct or wrong I am not talking about that correct and wrong accuracy will measure but how quickly you are predicting that speed will measure but once the prediction is done can you understand how did the model arrive at a particular prediction class of prediction that is called interpretability right now these are the few things and then there is something called scalability
interpretability and interpretability what do you mean by scalability right if your model is trained to handle 100 testable per minute and it is given 100 testable per minute and it is given 100 testable per minute can it handle that that is called scalability so how does the model perform on a larger data set is called scalability and interpretability we saw robustness means whether my model classification model can handle noise or not can handle noise or not
so these are the different performance matrices right so these are the different performance matrices right common issues we will see in the next video in this video we talked about various performance measures focusing on accuracy speed and interpretability we will continue in the next video take care bye bye
