Hello to all of you. My name is Dr. Hemant Rathar and we are in the course Machine Learning. We are
discussing module 4 - Decision Trees. Today's topic is from induction to application focusing
on pruning, overfitting and advantages of decision tree. So the learning objective of
this video is we are going to first discuss the stopping condition for building the decision tree.
We are going to look into overfitting and underfitting issues and then we are going to look into the practical advantages of decision trees. So let's start right away. So when we are talking about how to construct decision tree, the process is called as tree induction. So the process of tree construction is called tree induction and there are three major things we have to think about while doing tree induction. The first thing is how to choose
best attribute. So attribute selection question is the first thing we have to think. Second thing is how do we split a particular attribute and the third one is what is the stopping criteria. So in the last video we talked about how do we select the best attribute for tree construction. So we select the best attribute based on entropy or based on impurity calculation. That means if I split the parent based on a particular attribute based on entropy or based on impurity calculation. That means if I split the parent based on a particular attribute.
parent based on a particular attribute, the resulting childhood should be homogeneous in nature. That is the aim. So you calculate the information gain or gainy split or misclassification error for all the attributes and then find out which is the best attribute that should be used for tree construction. For splitting condition, it depends on the attribute type. And the third question which we are going to answer in this video is about
stopping condition. So let's look into one of the algorithms which is implemented for decision trees. The name of the algorithm is ID3. So decision trees again is a concept and it is implemented in various platforms. One of the platforms or one of the version which is implemented is called ID3. Right. Now this is not the only one. There are various variants of decision trees available but let me talk about the simplest version now in this video. Right. So we
build decision tree. So we are talking about how to construct the tree. Building the decision tree is a top down fashion. It's a top down. We follow a top down approach. That means we start from the root node. We go towards the leaf node. So we start from root node and we want to reach leaf nodes. Now when we are at the root node. Right. We have to select best attribute to spread. We have to select best attribute. Best decision attribute for spreading. Assuming A is the
attribute based on which i decided to split so if a is decided then we are in step number two a is
assigned as a decision attribute for the node now i'm going to split based on a so for each value of
a right new descendants of node are created right so child nodes are created some of the tuples of
this parent will go in child c1 and resultant tuples some of the tuples will go in resultant
child c2 sort the training examples in the leaf node right so you're going to arrange the examples
in leaf node c1 as well as in c2 once you are done with that then you check these child nodes
if these child nodes are perfectly able to classify that means they're homogeneous that means
the node impurity is zero then we stop if the resultant child node is not pure is impure we
iterate we again spread based on the best attribute at that time again we get resultant child node
again we are going to calculate the best attribute for this case right at this snapshot as you remember
decision tree follows a greedy strategy that means when i'm here i calculate assuming i'm doing information
gain calculate information gain of all the attributes locally i found that attribute a is the best attribute
so i split based on a now when i'm here at this child node again i'm going to calculate information gain of attributes
highest information guy attribute would be selected and there would be resultant child nodes so it follows a greedy strategy what is the greedy strategy here we select based on local maxima right we want to minimize the error in the child nodes or impurity in the child nodes there is no backtracking here as well right so once i decide based on entropy or information gain and i split
later i find out that this is not the best attribute and i should do it with b i should split attribute b at the child at the root node i cannot do that even if the tree is not getting global optimum
so this is the decision tree algorithm right so whatever we have discussed in the decision tree the first thing is the full data set is visible at the root node then at each node we follow a greedy strategy to find out best attribute to split on
we split the attribute we split the node based on the best attribute we found which is might be maximum information gain if you are using entropy and so on and then you do weighted calculation as well
you split based on that and then you do this process iteratively recursively on each child node till
the stopping condition how long we should split a resulting child node so there are three stopping conditions so how long i should split the resulting child nodes basically there are three stopping conditions one is you reached the stopping condition one is you reached a pure node that means you can reach the homogeneous
child node that means the impurity of the child node that means the impurity of the child node is zero if the impurity of the child node is zero that means you have reached a decision point so you can stop expanding a node when all the tuples or records within it belong to the same class so if i'm splitting this and the resulting child node is having all the tuples of only c0 class this is a pure node i have reached a decision point i will stop here right
the second stopping condition is when there is no more split possible and that can happen because of variety of reasons
if no attribute provide a significant gain or reduction in the impurity to justify the further split
they can stop so for example if i'm splitting here if i'm splitting at this level and i calculated information
gain of all the guys and i found that information gain reduction right there is very less entropy reduction
from this level to this level this is what we are doing entropy of parent minus entropy of child so entropy
reduction is very very less that means information gain value is very very less and it is not crossing a
threshold i can say that i am not able to take any decision here i am not able to take any decision here and i can stop
right so i can stop if no attribute provides significant information gain or reduction in the impurity to justify further splits in this scenario record can become indistinguishable right from the algorithm point of view that means i cannot split the record i cannot do proper classification at this point of time so for this child node i can say that that there are some samples of c0 class there are some samples of c1 class
c1 class decision tree saying that i cannot distinguish now i cannot do any prediction from here right this that happens more often than you can think and that also is a position where we stop right and this third stopping condition is early termination right we manually stop the decision tree construction also known as something called pre-pruning stop the growth of the decision tree early right so stop expanding the nodes it doesn't meet the threshold criteria so what can be the different threshold
criteria there are many i have listed there are many i have listed few of them and there are few more stopping conditions i have not listed them here but broadly there are three stopping conditions criterias and the for early termination these are the reasons right the first reason is minimum samples for the split think about it i am here this is my parent node so there was some root node split split split split and this is my parent now this is my c0 class this is my c1 class this is my c1
class there is one sample of c1 class there is one sample of c0 class there is one sample of c1 class right so there are total two samples in this node right so if there are very less number of samples in a node does it make sense to split that even if you get perfect classification after splitting like this right so this is a homogeneous node here this is a homogeneous node here is it a good way of doing it
i suppose no i suppose no why i am saying that think about it for a second why this is not a good way because maybe this particular tuple is noisy if this particular tuple is noisy and i reach here by traversing the decision tree for during final prediction if i reach here i will do classification based on a noisy sample so if there are very less minimum number of samples are very less at the splitting node that also becomes a question where you can
you can stop maximum tree depth right so if you can stop maximum tree depth right tree depth you are creating the tree what is the depth of the tree sometimes we stop tree construction after nth level right again so that the tree become doesn't become too big what happens if tree becomes too big the number of checking while doing finer prediction also increases think about it so if the tree is small height 5 maximum i have to do 5 comparison if the tree depth is 500
maximum i have to do 500 comparison so if i have to do 500 comparisons that is a costly affair that is a costly comparison comparison is not free why we are doing comparison at each decision node during prediction i am doing comparison right so if i have to do 500 comparison is a costly affair sometimes we want to control that by fixing the maximum tree depth and a third condition which we discussed earlier as well which is minimum improvement over in the impurity of the child node
right so don't split right so don't perform the split right if it reduces impurity at least by a certain amount right so if it is not reducing the impurity by a certain amount i can stop so information gain reduction is very very less if information gain is very very less i can choose to say that okay there is no much information at this child node i do not want to split it i do not want to take any decision here and we do that we do that that is a well-read argument rather than having a improper tree we create a journalization
tree and let me talk about what do you mean by generalization now so we have discussed these terminologies in the past as well let me discuss them again right in some detail so let's suppose let me plot a graph of on x axis i have model complexity
and on y axis i have error as we have discussed earlier there are two types of error train error and test error
what is train error when you train the model on training data and you test it also on training data
what is test error when you train the model on train data but you test it on test data so when i plot train error and test error in this diagram how it would look like so let's plot train error first if i plot train error it would be something like this this is my train error right and how test error would look like
this this is my train error this line is itself is a test error right we have discussed this let me elaborate again so when i am talking about model complexity in discient trees how can you implement model complexity i can implement model complexity by various ways one of the ways is tree height if the tree height is unbounded the depth is very high right i am allowing any level of depth then the model complexity is very high if the tree depth is very high right i am allowing any level of depth then the model complexity is very high if the tree depth
is fixed to be fixed to be fixed to be fixed to be 1 right the model complexity is very less so this is one way to implement model complexity in discientry is by controlling the depth of the tree right so how do we interpret this error train error right at when discientry of depth 1 is allowed only discientry of depth 1 is allowed train error is very high test error is also very high this is the point right i
this is called under fitting right this is called under fitting right that means your data set is complex but your model is too simple your model complexity is very less your model complexity is very less your model is not able to learn the data properly and that is why you have high train error and higher test error this is called under fitting there is another term called over fitting this zone is called over fitting what do you mean by over fitting?
over fitting means that you have over fitting means that you have over learned the data you have learned noises you have learned anomalies outliers etc you have over learned the data now what is the problem with over learned data or over learned model the over learned model gives me very less train error that means on training data set right it on training data set the error value is very less so you can see this is my train error but as soon as you change the data set the test data set from train data to test
test data right your test data right your test data right your test data right your test data right your test error becomes high so that means if you change the data set and then try to use the model the error will be high so you have over learned the data your model works perfectly fine on the data set on which you have trained but as as soon as you change the data set a bit your error starts to pick up which is called test error and that also is a bad scenario so there is high variance this zone is called high variance and
low bias area high variance means you change the data set the error is changing varying a lot low bias means that depth is not restricted i can make very complex this is a high bias area so now you can understand that neither over fitting not over fitting and under fitting both are not good so your dissonantry should lie somewhere here and this is the sweet spot so that is why to get an optimal very journalized
is the term properly journalized is the term properly journalized dissonantry you should train the dissonantry till a certain height or you can control by minimum number for split or you can do by minimum improvement in the threshold performance right so these are the different ways through which you can reduce the over fitting issues in dissonantry so your ideal place till which you can train your dissonantry should be here and this
is what we discussed right that there is something called train error there is something called test error and when you have height train and test error right in this zone this is called under fitting this zone is called over fitting and your model should be trained somewhere here this is the box we are looking for right but there is some sort of overlap between train and test error there are various ways of implementing this right height of the tree as i said or number of samples at the leaf node number of samples at the non leaf node all these things you can do
what is the problem with over fitting right over fitting means that you are over learning the data even if there is only one record you are trying to learn that if you are trying to learn that that means at leaf node you have only one sample and then you are trying to do classification for that if that leaf node is a noisy node it's a bad node noisy data then it is a problem all right you do misclassification because of that right so this case we want to avoid over fitting is what you want to avoid right over fitting is what you want to avoid right over
so this case we want to avoid right over fitting very simple analogy can be right a student try to memorize everything exact answers right so by conducting the exam right before conducting the exam we give practice exam and student what is trying to do is he memorize exactly memorize the practice exam but in real exam right in real exam i changed the question slightly so i'm changing the training test data set right so
from training data set i'm measuring the performance test data set i changed the data set now and the student is not able to learn anything or write anything in the answer book so you want to get into something called sweet spot right so you can do pre pruning etc etc to reach the sweet spot avoiding over fitting issues right so what are the advantages of decision tree we have finished decision tree now what are the advantages it is relatively inexpensive to construct compared to other algorithms you will find
that the complexity of the complexity of the complexity of the complexity of the decision tree is pretty less right it is very less i will ask you to do a homework can you write mathematically what is the complexity time complexity of decision tree that is your homework second advantage of decision tree is extremely fast for unknown samples right so it can do easy prediction easy to interpret and for a smart so for in very simplistic terms right once the decision tree is constructed it is
very easy to interpret how the final predictions are done and it is extremely fast to do predictions how it is fast you have to just traverse the tree towards one of the leaf nodes and that if you reach a leaf node you are doing the final prediction extremely fast decision tree is extremely fast and the path which you have followed can help me in finding the interpretation of why did i arrive to a particular decision accuracy of decision trees are very comparable to other classification algorithms as well so thank you for attending the
class. In summary, we discussed in this video about the stopping conditions in decision tree,
we looked into overfitting and underfitting issues in decision tree and we also looked
into why decision trees are so popular. Thank you for attending today's class. Take care.
Bye bye.
