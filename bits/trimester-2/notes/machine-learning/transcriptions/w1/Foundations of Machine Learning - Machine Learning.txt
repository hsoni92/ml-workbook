Hello to all of you. My name is Dr. Hemantra Athar and welcome to the Machine Learning course.
Today we are going to start with Module 1: Introduction of Machine Learning.
This video is focused on Foundations of Machine Learning.
So the learning objective of this video is first we are going to define what Machine Learning is and its relationship to Data Science and Artificial Intelligence.
Then we are going to identify the key stages of building standard Machine Learning workflow.
So let's start right away. Why do we need Machine Learning?
Today data is constantly being collected, generated and collected from countless sources.
Think about a scenario. 10 years back, when you want to take a photograph, you will take a physical camera, take a photograph and take a printout.
So that was a physical experience.
Today if you want to take a photograph, you will start your mobile phone, take a picture and store it on a cloud.
So that physical experience today is converted into digital experience. And this is not only true with photographs, but with various other processes in our life.
For example, today in businesses, a lot of physical experiences are converted into digital experiences.
For example, in e-commerce, previously if you want to purchase a book, you will actually go to a book store and then purchase a physical book.
Today if you want to purchase a book, you go to Amazon, Flipkart and order the book online.
So again, a physical experience of purchasing a book converted from physical experience to a digital experience.
Earlier, if you wanted to search a topic, you will physically go to a library, open a few books,
physically search for a particular topic and then read the topic.
Today if you want to read or understand a topic, you go to web and then search it on Google or some other platform.
Right. So a physical experience again converted into a digital experience.
Stock market trading. Earlier if you want to purchase a stock, you will go to a stock broker and physically do the transaction.
Now if you want to purchase a stock, you will open an application, right, like Zerodha or anything else, right, and will purchase a stock.
So a lot of these physical experiences are now converted into digital experience.
Because of this, now data is constantly being collected from various sources.
Now the data storage capability has also increased drastically.
20 years back, if I want to store a data, right, it was very costly.
Even the storage capability was very less.
For example, when I first bought my first machine in 2000,
the disk HDD capacity was 6 GB or was 8 GB.
Today I have a phone which has memory in terms of 128 GB.
And I have a machine which has storage capability in petabytes, right.
So a lot of physical experiences has been converted into digital experiences.
The data which is getting generated, now storage has also improved drastically.
And which has led to large volume of data being generated.
When we have large amount of data, we are drowning in data.
If we start asking questions from the data, we are not getting it.
That is why we are starving for knowledge.
So data generation in other areas is also increased, right.
For example, today because of scientific simulation, bioinformatics, remote sensing,
lot of data being generated.
In society also, earlier if you want to communicate, right, you will physically go to someone's house
and then talk.
Today you have social media which can enable digital form of communication.
Earlier if you want to read news, you will have a physical newspaper,
read the news, you go to google news and read the news and so on.
So a lot of these physical experiences now is converted into digital experience.
These digital experiences constantly generating very large amount of data.
The data storage capability has also increased drastically.
Therefore, we are drowning in data.
But if I start asking relevant questions from the data which we are having today or which is
getting generated today, we are not getting proper answers.
For example, right, in e-commerce, right, let's suppose you have done a lot of banking transaction
or you have purchased a lot of books.
If I ask relevant questions, for example, how many were actually fake bookings,
fake order, e-commerce websites are struggling to build systems that can detect fake users or fake
orders or fake bookings, right.
So we have tons of data.
But if we start asking relevant questions, right, we are not getting proper answers.
Another example can be stock market, right.
We are generating tons of data in stock market.
But if I ask which stock I should purchase tomorrow, which stock I should sell day after tomorrow,
I am not getting relevant answers from it.
So we are starving for knowledge, right.
So we have tons of data.
And if we start asking relevant questions from the data, right, we are not getting proper information.
And that is why we are starving for knowledge.
That is why machine learning is so prominent today.
Why?
Because we have tons of data.
We are able to store it.
Maybe we are able to process it as well.
But if we ask for relevant questions and try to get relevant information from it, we are not getting it.
And therefore we are starving for knowledge.
Now if you look into the history, right, the technology of data has drastically changed last 500 years.
Earlier, right, we believed in empirical sciences.
For example, we believe in ideas based on observation.
For example, right, earlier, right, in 1600, right, we will wake up in the morning and we will look towards the east for sunrise.
And in the evening, we will look towards the west for sunset, right.
So this is an observation that sun will always rise from east and will always set at west, right.
So this is an observation or based on the observations, we were building empirical science ideas or models.
So this was the age from before 1600.
It was an empirical science age where lot of ideas were driven from observation, were based on observation.
Then came the age of theoretical science.
From 1600 to 1950s, the age was of theoretical science where scientists, based on the observation,
will actually mathematically build models and will try to generalize the idea.
Think about a scenario, right. Sun always rise from the east, but throughout the year, not at a particular time.
In winters, sun will rise late and in summer, sun will rise early.
Similarly, right, sunset, right. In winters, sun will rise early and in summers, sun will rise late, right.
So now, during the theoretical age, scientists will try to write mathematical formulations or models and will try to
generalize the bounds and this was the age of theoretical science.
Then came the age of computational science.
When computers were actually designed and developed,
the age of computational science came where we had some limited computational power, right.
We had calculators, we had limited computational machines, which can process data at faster rates.
During this duration, humans will do the calculations. But now, from 1950s, 1990s,
computers start coming into the picture and we had limited computational power, so that whatever
models we have built, right, we will trying to run simulation and we were trying to develop even
more complex models, right. So this was the age of computational science. And today, we are in the age of
data science and machine learning where the computational resources or computational power has drastically
increased as well as computer storage or data storage capability has also increased. So now,
I have higher capacity to compute, I have higher capacity to store and I have higher capacity so that
these two can communicate with each other. And therefore, now we, whatever data earlier we used to observe,
now we are actually digitalizing it, storing it, and then we have computational resources so that we can mine
information from it, we can learn pattern from it. And that is what we are trying to do with machine learning algorithms.
Machine learning algorithms actually try to extract patterns from the data which is stored, right.
So based on the higher computational resources, we have higher storage resources we have. And the good communication channel
between them now is the age of machine learning and data science where we are trying to write algorithms that can extract or uncover patterns from huge amount of or from massive amount of data.
So what is machine learning? Machine learning is a field of study
that actually uses computer to learn the data without explicitly being programmed, right. So earlier if I want to actually build a model, I will write mathematical formulations or exact rules that will drive the formula. But today, I do not have to write those rules. Machine learning algorithms will automatically learn with those rules or patterns without being explicitly programmed.
So machine learning. So machine learning extracts ideas, information from the data that we have. We do not have to program it. We do not have to tell the machine what to extract. It will automatically extract knowledge from it.
It is evolved from the field. Machine learning has evolved from the field of data mining
where the focus is to extract useful, previously unknown knowledge from the larger data set. Right. So machine learning has evolved the ideas from data mining which focuses on extracting useful, previously unknown knowledge from the larger data set. Right. So here in machine learning, we discover meaningful patterns automatically or semi-automatically.
And this is the machine learning pipeline. You have seen this pipeline in the data pre-processing course as well. Right. So if I quickly want to revise that, initially you start with data and set of questions.
So I have tons of data and I have a question. Whether it is going to rain tomorrow or not. Right. This is the question I start with and maybe last 10 years of weather data. Once I have the data, first I have to do data selection. So I have tons of data. I want to select a subset of data so that I can answer this question. Processing whole data is costly, time consuming and might be noisy. Right. So that is why I perform data selection.
First, once data pre-processing. First, once data pre-processing, you have the targeted data, then you perform data pre-processing on the targeted data. In data pre-processing, you can do data cleaning, data transformation, removal of noise, etc. etc. Once data pre-processing and data transformation is done, then we actually apply machine learning and data mining algorithms on it, on the transformed data.
Once that is done, once that is done, we have some results. We have interpreter and evaluation tool to look at the results or performance matrices to look at the results. And then based on that, we will extract or get knowledge from it. Right. So what do you mean by knowledge?
If you can answer, if you can answer the question you started with, right, from the knowledge which you have gathered, then this pipeline is successful. Other file, the pipeline is broken. You might have to revisit one or more stages to fix the pipeline so that you have got proper knowledge so that you can answer properly, right, the question you started with.
So, for example, when we are talking about whether it is going to rain tomorrow or not, if the knowledge is that yes, it is going to rain tomorrow and it rains tomorrow, right, then the knowledge extracted from the previous data was good.
So, what is the origins of machine learning? Machine learning extract ideas from various domains. It is not a single discipline. It draws ideas or strength from various other fields like statistics and AI, machine learning and pattern recognition and database systems. Statistics and AI, right, it learns theory and predictions. Right. So machine learning or data mining is the same thing. It is not a single discipline. It draws ideas or strength from various other fields like statistics and AI.
AI, machine learning and pattern recognition and database systems. Statistics and AI, right,
it learns theory and predictions, right. So machine learning or data mining extract ideas
from statistics and AI, patterns, recognition and database system. Now let's look into the machine
learning workflow. Machine learning is not about running one single algorithm, right. So a lot of
time you will see that people run maybe addition tree build, addition tree model and they say that
they are doing machine learning. It is not machine learning. Machine learning is a comprehensive
multi-step process, right. And the two popular workflows we are going to see today which can
help us in building a machine learning pipeline, a machine learning workflow. So KDD is one of the
ways we can build this pipeline or workflow. KDD stands for knowledge discovery in databases.
And there is another industry-wide pipeline which is called CRISP-DM. So what do you mean by KDD process view, right. So basically what we want to do is we want to use machine learning to build a solution, right. To build some answers, to extract knowledge from the data. And for this we have a pipeline, right. So the KDD pipeline starts with the raw data, right. And will end with the knowledge. So as I said, we have seen this pipeline earlier.
As well, we start with set of questions. And data. And data. A group of and data, right. So in databases, the first stage, in first stage, we have raw data, right. We have raw data. Weather prediction system. If I'm building, I have last 10 years of weather data. On a particular date, what was the temperature, what was the humidity, what was the wind speed, what was the wind direction. All this data, I'm extracting and I'm storing, I'm gathering and I'm storing in a database.
This is called as raw data source. Once you have this data, right. The second step here is data integration and cleaning. So you might be storing data, the original data in various data structures. First thing is you have to integrate that data. Multiple tables need to be combined together so that you can build a model later on. So data integration is merging, combining data from various sources. So in the second step, you are doing data integration.
where various databases or data sets are merged together and what you create is a data warehouse
so now in the third stage you have created a data warehouse so in data warehouse you have data from
various sources one more thing you do in stage two is to perform data cleaning or data pre-processing
in data pre-processing we can do various activities like data cleaning data integration
right outlier detection etc etc all these things we perform in data pre-processing so that data is
ready for the next stage so in data warehouse we perform data selection in which we select task
relevant data so from data warehouse which has data coming from various sources we select a subset of
data which is called as task relevant data once we have the task relevant data then in the fourth stage
we will build a machine learning model or a data mining model i'm using machine learning and data
mining model interchangeably but later on in other courses you will see there's a bit difference from
both of them but that's fine today you can imagine or assume that data mining and machine learning are
same but later on we will discuss how they are different so in data mining or machine learning
algorithms will be used to find patterns here so once you have task relevant data now i will build a
model now i will apply a data mining algorithm that can extract knowledge or patterns from the data which
we have once we have extracted the knowledge in the fifth stage right we are going to evaluate the
patterns which were extracted so here you will identify the patterns which were extracted and then look
and measure the quality of patterns which are extracted so quality measurement is done here right so whatever
patterns you have extracted whether it is a good quality or not and once that is done in stage number sixth
we will extract knowledge from it right to find the final and actionable insights are
we will received at the knowledge stage stage number six in this stage right if we are able to
answer the questions we started with then this pipeline is correct otherwise we have to rerun
the pipeline maybe change the algorithm data mining or machine learning algorithm maybe change the
data selection technique maybe do data integration data cleaning again to get the proper results get the
proper knowledge which can answer these questions similarly we have crisp dm pipeline right so this
is more of a general pipeline knowledge discovery data in databases is a general pipeline in industry
it has been modified a bit and then adopted industry so crisp dm pipeline is a popular framework used in
the industry for building machine learning projects right here also you start with data prior knowledge
and questions right so you start with data some domain expertise and a set of questions once that
is done you are going to integrate the data from various sources you're going to prepare the data
from various sources so when we are talking about data preparation right we might do data integration we
might do data cleaning we are basically preparing the final data so once data preparation stage is done
step number two is done in step number three we model we build a model so here we use various machine
learning algorithms to build the model so here we select and apply machine learning algorithms in the modeling stage
stage then comes stage number four where we actually apply the build model which we have
built in step number three on a newer data and evaluate its performance evaluate the performance
of the model which we have built in step number three so we apply whatever model we have built
we apply the model on a newer data and check whether the performance is good or not once that
is done then we extract knowledge in step number five if the model is performing successfully is
performing good then we extract knowledge from it and check whether we are able to answer the
questions or not if we are able to answer the question the pipeline is successful otherwise
pipeline is broken similarly we have business bi business intelligence view of the kdd pipeline or
the machine learning pipeline we start with some data sources we first do data pre-processing
including data integration and creating data warehousing then we perform data exploration then
we apply machine learning and data mining algorithm on it then we present the data and finally we make the
decisions so again this is also a five step pipeline this is three this is four and this is five very
similar to what we have seen earlier so what kind of data i can use in machine learning or data mining so we
can either use structured data data arranged and stored in a structured way so we can actually use structured data some machine learning algorithms also works on unstructured or semi structured data where the structure is not proper for an example of structure data is a tabular data or a graphical data these are example of structured data unstructured data that means you have a news article you have a web page this is more of an unstructured data.
This is more of an unstructured or a semi structured data we can direct apply machine learning algorithms on it or convert these unstructured or semi structured data into structured data and then apply machine learning algorithm on it. Both types of applications you can see in real one. Third type of data is called time series data. Time dependent data. There are some advanced data types as well. For example data stream or sensor data data. Data flowing in real world. Real time data data.
Real time data from IoT devices is called stream data. Streaming data. Time series data. Socio temporal data and world wide data. These are some advanced type of data we can use to build the model. So in summary in this video we looked into basic definition of machine learning and we also looked into the workflow to build a machine learning pipeline. We also looked into different data types used in machine learning. We will continue in the next video.
Take care. Bye bye.
