Hello to all of you. My name is Dr. Hemant Rathaur and we are in the course Machine Learning. We are
discussing module 2 Data Pre-processing and this video is about data quality issues. So the learning
objective of this video is we are going to first define data quality and look into its issues. We
will look into the key dimension to measure data quality like accuracy, completeness, consistency.
Then we will look into common data quality problems like noise, outlier, missing value, duplicate value,
inconsistent value etc etc. So let's start right away. So the aim of this Machine Learning course is
to build a model right maybe a prediction model maybe a description model. So before building the
model we must be gathering the data. When we are gathering the data in ideal world we assume that
the data which we are gathering is perfect, complete and ready for analysis. This is an ideal world but
if you look into real world whenever you gather a raw data it is often very messy, incomplete and
inconsistent. So in ideal world we assume that the whatever data we are gathering is perfect. It contains all the
information and it can readily be used for building a model. However, when we actually do it in real world, raw data is often messy, incomplete and inconsistent. Now in real world this is a scenario that is why there is a fundamental principle in data science which is garbage in, garbage out. If we have poor quality of data, the model which we are going to build on it,
So if we have poor quality of data on it, maybe a prediction model, maybe a prediction model, maybe a description model will also be of poor quality. So if it is a prediction model and raw data is of poor quality, the predictions will also be of bad quality. So if we feed a model a low quality data, then the results will also be of low quality or unreliable. So the fundamental principle in data science is garbage in, garbage out. Low quality data will lead to low quality data. So the fundamental principle in data science is garbage in, garbage out. Low quality data will lead to low quality data.
Low quality of mining results. Now when we look into data quality, what is the definition of data quality? Data quality refers to the overall utility of the data set and its ability to be easily processed, analyzed for the intended purpose.
So let's suppose I want to build a credit card for detection system, right? For that I should have log of thousands of customers doing banking transaction or credit card transaction and with proper labels. If that is not there, then the data is of poor quality.
So a golden rule in data quality is data has quality if it satisfies the requirement of the intended user. So a high quality data correctly represents the real world construct as it is meant to be. So as I said, data has quality if it satisfies the requirement of the intended user.
So for example, the same data set might be of high quality to the same data set might be of high quality to the one user. For example, R&D manager, etc, etc, etc. And will be of low quality to another manager, right? For example, let's suppose I have a data set, right? A tabular data set from Amazon.
Now, this is the Amazon which is gathering a lot of data from its customer, right? Now, for R&D manager, this data is of good quality. Why? Maybe because the type of question which R&D manager is going to ask from this data set, we are able to satisfy, we are able to get the correct answers, right? So data quality is very subjective. If data is of high quality, if it satisfies the requirement of the intended user. So what R&D manager is going to answer, ask from this data set, it is going to answer.
Ask whether the product has innovations or not. Whether product has sort of what is the new innovation than on a product. So all these questions, if I'm able to answer from a particular data set, then the data set is of high quality. Otherwise, data set would be of low quality. For example, delivery manager, what sort of question delivery manager will ask from the same data set? It will ask whether the products were delivered on time or not. So the same data set might be of good quality for one user and might be of bad quality for another user.
Now, when we talk about data set, it can be measured using various measures. It's a multi-dimensional concept. And the key dimensions include, right? So these are not the exhaustive list, but the key dimension to measure the quality of the data is accuracy, completeness, consistency, timeliness, believability, and interpretability. So these are the some ways to measure the quality of the data.
And then we can define, we can say that the data is either of good quality or of poor quality. For example, I can measure the data quality using accuracy. So what do you mean by accuracy? Is the data correct or not? Right? So is the customer name spelled correctly in the data set or not?
If the customer name spelled incorrectly in the data set. Data set is not accurate. And if data set is not accurate, whatever predictions I'm going to build, do on the build model, right? Would be also be of bad quality.
Data quality can be of bad quality. So if the customer name is, is all the necessary information or data present in the particular data set or not? For example, is the address field filled out completely or not? So if the address is incomplete, data is of poor quality, right?
Completeness is another parameter to define the data. Data is consistent or not? Does the data contradict itself or not? For example, I'm gathering customer age and his or her date of birth. If I can easily find or calculate with the help of date of birth, what is the customer age?
If there is a mismatch, if there is a mismatch, then it is called data is inconsistent. Data is not matching or contradicting itself. Data should be timely, right? Timely means is the data up to date or not? Whether the current address of the customer is correct or not? So it should be updated on a timely basis. So timeliness is another parameter that can be used to define data quality.
Data should be believable. How much we can believe on the data and data should be interpretable. By looking at the data, can we understand the data set easily? That is called interpretability. So data quality can be defined using six parameters and there are few more. But these six parameters are accuracy, completeness, consistency, timeliness, believability and interpretability. So any data, right?
Whatever data you are gathering from real world, you can measure the quality of the data based on these parameters. Now, what are the common data quality problems? So data quality problems include data might be noisy, data might have outliers, data might have missing values, data might have duplicate data, right? So the dimension of data quality give us a framework and that through that I can measure whether data has quality problems or not.
So what do you mean by noisy data? So what do you mean by noisy data? Noisy data means noise refers to a random error or a variance in a measured variable. Modification in the original value is called noise, right? So let's suppose I am talking on a phone in a bad weather day, right? So there will be a lot of disturbance when two people are communicating on phone on a bad weather day.
And that is why because noise is added up in the channel and that will lead to distortion and noise at both the ends, right? On the phone. So think of this as a static or distortion that is added in the true signal in the data set, right? This might happen because of faulty data connection, right? Data transmission issues, data entry problem.
For example, for example, for example, for example, for example, for example, census, we are doing census now. A government of India has decided to do census now. In census, government employees or vendors will come to your house and ask for data. They're going to ask how many TVs you have? How many two-wheelers you have? How many four-wheelers you have? Now, if someone maliciously give wrong information, right? Then it is called noisy data, right?
So they will be noting down the data when they are coming to the house. Government employees coming to your house, right? They will be noting down the data in a register that register will be digitalized. So there will be a data entry operator setting in the model. So while converting the register data to a digital form, there might be data entry problems.
Or while collecting the data itself, right? Or while collecting the register itself, right? Noting down the register itself, vendor, government deploy has done some mistakes. So that will also lead to a noisy data or data entry problem. Transmission error. I have already given an example. When you speak on phone on noisy day, then it is called transmission error or transmission noise.
Now, second problem in the data set can be right or data quality issue can be outlier. So what are outliers? Outliers are data objects that are legitimate, but have characteristics that are considerably different from most of the other objects in the data set. So outliers are data objects that are genuine, that are legitimate, that are true, but have characteristics that are considerably
different from most of the data points in the data points in the data set. For example, if this is a two-dimensional data, if you look, right, all these points are bunched together, all these points are bunched together, all these points are bunched together, all these points are bunched together, but if you look very careful at this point, right, this is an outlier. It is legitimate, but it is considerably different from rest of the points in the data set. Similarly, about this data set, this is another data, this point and this point. So this is the outlier one,
this is outlier two, this is outlier three, this is outlier three, and so on. So an important thing to note here is noise is an error, but outlier extreme, but real value are legitimate value are true value. Right. So what are the two issues we face when we interact with outliers? Outliers can create a nuisance, right? And it is very important to discover them. Why? Because outlier can
sometimes create a nuisance can sometimes create a nuisance and can sometimes be a goal, for example. Outlier are genuine points. When can they behave like a nuisance? Right. Think about it. Let me take a simple data set. Data points are 1, 2, 3, 4, and 500. Or maybe 100. Now, let's suppose this is my data set. Now, if you look very carefully, right, there is one outlier
outlier in this data set. This is the outlier. Now, if I include this outlier in the data set, right, what will happen? It is changing or distraughting statistical analysis and models in my data set, which I am going to build on the data set. For example, right, if I take average of this data set, right, what will be the average? It would be 1 plus 2 plus 3 plus 4 plus 100.
right, so if I do that, right, so if I do that, right, 6, 10, like this, right, so 22 is the average. If I remove this outlier, what is the average?
And without outlier, what is the average? And without outlier, the average is 2.5. If you look very closely into the data set, right, all the points when you see are much closer to the average, right, if I remove the outlier, right? So average as a statistical measure is giving me incorrect picture if I have outlier in my data set, right? I gave you a very simple example.
If I have outlier in my data set, if I have outlier in my data set, a parameter called average will start giving me incorrect picture about the data set itself. With outlier, the average of the data set is 22 and without average, it is 2.5, which is much real.
And talking properly or talking properly about the representing properly the original data set. Sometimes outlier is a goal we handle, right? It is an important point to discover, right? Sometimes finding an outlier is an entire point of the data set. For example, credit card fraud detection, inclusion detection system.
So credit card fraud detection, what exactly we are going to do, right? Different customers or customers will scan their credit card, will swipe their credit card whenever they are doing the transaction. Every day, millions of transactions are done. A handful of them are bad, are malicious and we want to detect them.
So that is called credit card fraud detection. Credit card fraud detection means whenever you swipe your credit card and if it looks like a fraudulent activity, you should be able to detect it. Intrusion detection system. Millions of packets flowing inside my network, going out of the network. Out of millions of packets, only one or two or maybe a minor number is malicious in nature. Rest all are good. I want to detect them. That is called intrusion detection system.
So these frauds, these intrusion packets are essentially outliers and we want to detect them. So sometimes outliers create nuisance. Sometimes detecting outlier is a goal, right? So in that outlier we have to be careful, right? Another problem with data quality might be of missing value. Some values are missing. It is a common problem in the data set when there is some missing information is there in the data set.
So data might be missing because information not collected, attribute not applicable or some equipment was not working, malfunctioning was not working properly. Another issue with data quality is duplicate data. That means, right? Two attributes or two objects are very similar, are duplicate or near duplicate to each other.
This happens when we merge data from multiple sources. And if you do not do carefully, right? Redundant data would be there in terms of both attributes, in terms of both tuples, right? And it might happen and then we want to detect it and handle it properly.
So in real world data is always of messy quality, right? So in real world data is always of messy quality, poor quality, bad quality. Data would be incomplete. That means there will be some attributes which are missing. Some are of aggregate value. Data will be of noisy, right? We'll have some outliers, errors, etc, etc. Might be inconsistent as well.
So real world data is always of poor quality. So real world data is always of poor quality. And the basic idea is, if you use poor quality data, then the model and the predictions done with the help of on the poor quality data will be also be of poor quality, right? So this will lead to bad business decisions, will lead to reduce in the trust on the system and will be waste of resources of both in terms of time and etc.
So why we do? Once we know data is of poor quality, then we perform data pre-processing as a solution. So since the raw data is always of poor quality, is of messy quality, might have noise, might have outliers, might have inconsistent data, we perform data pre-processing to improve the quality of the data.
So data pre-processing describes ways of processing the raw data and preparing it for the next stages of analysis. Data pre-processing describes steps on raw data to prepare it for the next stages of the analysis. So it is a preliminary stage and it is used to convert raw data into a clean and consistent data.
So data pre-processing has two goals. One is, it improves the quality of the data. It fixes the problem in the quality of the data, in the data quality which is, it handles noise, it handles missing value, it handles inconsistent data. We have seen solutions to improve the quality of the data. It modifies the data so that it can fit the model properly. So structure data, right? In a way that it is optimally fitting into one of the data models.
Data mining or a machine learning or a data science algorithm. And data pre-processing helps us in doing that. So in summary, in this video, we talked about data quality issues. Data quality can be measured using various dimensions like accuracy, completeness, consistency, consistency, and consistency.
We looked into common data quality issues like noisy data, missing values, duplicate data, and inconsistent data. We will continue in the next video as well. Take care. Bye-bye.
Bye-bye.
