Hello everyone, welcome back to module 3 of Artificial Neural Networks. In this video, we will discuss the multi-layer networks and XOR problem. By the end of this video, you will be able to explain why a single perceptron cannot solve the XOR problem, you will understand how a hidden layer enables multiple linear splits, and you will
finally recognize why multi-layer networks are more expressive than single-layer perceptrons. In the previous video, we saw that the perceptron fails on the XOR problem. This failure is not due to poor learning, bad initialization or insufficient data. It is a structural limitation of the model itself. A single perceptron can create only one linear decision boundary, one straight line in two dimensions,
or one hyperplane in higher dimensions. This single boundary can divide the input space into only two regions. However, the XOR pattern requires the input space to be split into multiple regions in a specific way. Since one straight line can never produce this kind of partition, a single perceptron is fundamentally incapable of solving XOR. So, what changes when we add
a hidden layer? The key idea is that a hidden layer allows us to create multiple linear feature splits instead of just one. Each hidden neuron can learn its own linear separator of the input space. These intermediate linear features are then passed to the output neuron which combines them into a final decision. By composing multiple linear cuts in this way, the network can form nonlinear decisions.
the decision regions in the original input space. This is the crucial representational leap. We are no longer restricted to a single straight boundary. The XOR problem can now be understood very simply. So, let's say if we have to classify the points for this XOR problem, what we can do is, we can create these two lines. Now, all the points which lie inside the two lines can be classified as the same.
as one class. So, XOR is solved by first creating two different linear partitions in a hidden layer. So, XOR is solved by first creating two different linear partitions in a hidden layer represented by these two lines and then recombining those partitions into a single final decision at the output layer. This requires at least one hidden layer. Therefore, the key takeaway is this.
Neutral layer networks are strictly more expressive than single perceptron. They can represent functions that no single linear model can. This increase in expressive power, not just better learning rules, is the fundamental reason why deeper neural networks outperform the shallow ones. Now, let's summarize the main points of this video. Solving XOR requires splitting the space in multiple regions.
This is not possible by a simple perceptron. A hidden layer helps to create multiple linear boundaries. This makes the multi-layer networks more expressive than single perceptrons. In the next lesson, we will introduce the logistic neuron which serves as a smooth, probabilistic building block for these multi-layer networks. Thank you.
