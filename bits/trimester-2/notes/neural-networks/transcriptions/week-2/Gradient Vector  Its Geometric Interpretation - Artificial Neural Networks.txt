Hello everyone, welcome back to Module 2 of Artificial Neural Networks. In this video, we will discuss what is a gradient vector and how to understand it geometrically. By the end of this video, you will be able to define the gradient vector as a collection of partial derivatives.
You will interpret the gradient as the direction of steepest increase of a function. You will understand its geometric meaning using level curves. And finally, you will be able to explain the role of the gradient in neural network training and optimization.
In the previous video, we learned about derivatives and partial derivatives. We saw that a partial derivative tells us how the output of a multivariable function changes when we vary one variable at a time.
Now, consider a real neural network. Its output depends on many variables simultaneously. Inputs, weights and biases. Looking at one partial derivative at a time is not enough.
We need a way to collect all these sensitivities into a single mathematical object. That object is called the gradient vector. Formally, for a function given by f , the gradient vector is defined as the vector del f equals del f by del x1, del f by del x2, so on till del f by del xn.
Each component of this vector is a partial derivative with respect to one variable. So, the gradient tells us in one compact vector how sensitive the function is to every variable at once. Notice that the gradient has the dimension of n which is the same as the input vector. This makes it mathematically and computationally very convenient.
The gradient has an extremely important interpretation. The gradient vector always points in the direction of the steepest increase of the function. Its magnitude tells us how fast the function is increasing in that direction. If the gradient is large in magnitude, the function is changing rapidly. If the gradient is close to zero, the function is locally flat.
When the gradient is exactly 0, the gradient is exactly 0, we are at a stationary point which could be a minimum, a maximum or a saddle point. These are exactly the points of interest in optimization and learning.
Let us revisit our function f , let us revisit our function f = x^2 + y^2. Let us try to compute its gradient. So, the gradient of this function would be del f given by a vector of del f by del x and del f by del y. Let's try to see what is the value of del f by del x. Here, the value of del f by del x is the value of del f by del x. Here, the value of del f by del y is the value of del f by del x.
Here, f equals x^2 + y^2. So, del f by del x is given by derivative of x^2 with respect to x which is 2x. And since y is constant with respect to x, so the derivative here would be 0. So, del f by del x is given by 2x. Similarly, del f by del y would be x is constant with respect to y. So, del f by del x is given by 2x.
Similarly, del f by del y would be x is constant with respect to y. So, del f by del y would be x is constant with respect to y. So, its derivative would be 0. Plus, derivative of y^2 with respect to y would be 2y. So, del f by del y is given by 2y. So, the entire gradient vector would be given by 2x, 2y.
del f by del y would be 2y. Now, imagine you are at point 1,1 and you want to move upwards in the steepest way possible. So, point 1,1 would be this point. Let's say you are here and you want to move upwards. Can you think of the direction in which you are going to move? So, the direction would be given by a tangent at this point. Which points in the direction? So, the direction would be given by a tangent at this point. Which points in the direction? So, the direction would be given by a tangent at this point.
which points in the direction of the gradient vector 2x, 2y. Now, there is something called as a level curve along which the function value is constant. Each curve corresponds to a specific value of the function. For this case, let's look at some of the points. So, for point 1,1 the value of the function is 1 square plus 1 square equals 2x.
points in the direction of the gradient vector 2x, 2y. Now, there is something called as a level
curve along which the function value is constant. Each curve corresponds to a specific value of the
function. For this case, let's look at some of the points. So, for point 1, 1, the value of the
function is 1 square plus 1 square equals 2. For point , again, the value of the function is 2.
Similarly, for , it's again 2 and is also 2. So, we see that across all these four points, the value
of the function remains constant. These points can be represented in a level curve which is shown
something like this. So, all the points which lie on this circle will have the value of the function
as the same. Now, there can be like multiple level curves here. So, here is the key idea. The gradient
vector is always perpendicular to the level curves. If you move along a level curve, the function value
does not change. But, if you move across the level curves, let's say from this curve to this curve and so on,
the function value changes and it changes fastest when you move in the direction of the gradient.
In neural network training, we do not try to maximize a function. Instead, we try to minimize
a loss function. To minimize any function, they must know in which direction the function decreases the
fastest. That direction is given by the negative gradient. Every modern learning algorithm, from simple
gradient descent to advanced optimizers, is built on this single idea. Use the gradient to decide how to update the
parameters. So, the gradient is not just a mathematical object, it is the driver of learning. Now, let us
summarize the main points of this video. The gradient is a vector of partial derivatives. It points in the
direction of the steepest increase of a function and the negative gradient points in the direction of the steepest decrease.
Gradient-based learning works by repeatedly moving parameters in the direction of the negative gradient to reduce the loss.
In the next video, we will study the chain rule which explains how these gradients are actually computed
efficiently for complex multi-layer functions such as neural networks. Thank you.
