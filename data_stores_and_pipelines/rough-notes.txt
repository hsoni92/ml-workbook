ecommerce .... sellers will be there... 


3 sellers ....

1... 123,124,125

123,iphone,500
124, earphone, 10...........>>>>>>> earphone..... revenue, profit
125, mouse, 15

2... 123,126,127

126............. abc........
124....

3... 124,128,129

=======
few sellers...... price...... mrp

====
internal....
competetor data..

abc... catalogue...list of items .... 

xyz.... 
===
UI... sellers .... onboard...catalogei

data ...... 

====
recommend such items to other sellers....

==

extract ,transform, load ..

dataset ... etl pipeline

3 etl pipelines...

===

100 records... dq checks.... 


80 records good data.... hudi
20 recors bad data.......path.... 

==


catalog | 123,flkfkfldl | reason..

extract .... input dataset.... keeping it one path ... path you will read it and you will maintain in a dataframe ... store in one temp location..



transform... reading broze layer data ..... cleaning... dq.....final data writing a dataframe or loaction


gold ..... reading this tranform output..... writing into final hudi table ...valie

in valid.... quarentize

====

3 pipelines



1... 5 marks 

pass some 3-10 input datasets .... program.... what is the expected recomendation.... 2 marks 

pass 3-10 input datasets...malformed data.... 2 marks ... 0 

coding standsrd.... 1 mark 

2. 5
3. 5 marks 

====

duplicate check ... 0 marks.... 

12 AM... 


==== consumption

extractor .... reading all 3 etl pipeline outputs
tranform.... recomendation
load... file file 

<rollnumber>/ecommerce_seller_recommendation/<s3 or local>/

12345/ecommerce_seller_recommendation/s3/
configs/

==

friday ... cor

sunday... not


pyspark... 

recomend.... only top 10 selling item to all the sellers..












