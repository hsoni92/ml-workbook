"""
Enhanced Model Drift Detection Test Suite Demo

This demonstrates advanced drift detection capabilities:
1. Frozen test suite regression
2. Semantic similarity comparison
3. Statistical drift detection
4. Multi-conversation context retention tests
5. Fact and keyword validation
6. PII (Personally Identifiable Information) security detection
"""

import json
import re
from typing import Dict, List, Tuple, Optional, Set
from datetime import datetime
import numpy as np
from collections import defaultdict


# Mock embedding function (in production, use sentence-transformers or similar)
def get_embedding(text: str) -> np.ndarray:
    """Simple mock embedding - in production use sentence-transformers"""
    # This is a placeholder - real implementation would use:
    # from sentence_transformers import SentenceTransformer
    # model = SentenceTransformer('all-MiniLM-L6-v2')
    # return model.encode(text)
    np.random.seed(hash(text) % 2**32)
    return np.random.rand(384)


def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:
    """Compute cosine similarity between two vectors"""
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))


class PIIDetector:
    """Detects Personally Identifiable Information in text"""
    
    # Regex patterns for common PII types
    PATTERNS = {
        'email': re.compile(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'),
        'phone': re.compile(r'\b(?:\+?1[-.\s]?)?\(?([0-9]{3})\)?[-.\s]?([0-9]{3})[-.\s]?([0-9]{4})\b'),
        'ssn': re.compile(r'\b\d{3}-\d{2}-\d{4}\b'),
        'credit_card': re.compile(r'\b(?:\d{4}[-\s]?){3}\d{4}\b'),
        'ip_address': re.compile(r'\b(?:\d{1,3}\.){3}\d{1,3}\b'),
        'date_of_birth': re.compile(r'\b(?:0?[1-9]|1[0-2])[/-](?:0?[1-9]|[12]\d|3[01])[/-](?:19|20)\d{2}\b'),
    }
    
    @classmethod
    def detect_pii(cls, text: str) -> Dict[str, List[str]]:
        """
        Detect PII in text.
        
        Returns:
            Dict mapping PII type to list of detected values
        """
        detected = {}
        for pii_type, pattern in cls.PATTERNS.items():
            matches = pattern.findall(text)
            if matches:
                # Flatten tuple matches (like phone numbers)
                if isinstance(matches[0], tuple):
                    matches = [''.join(m) for m in matches]
                detected[pii_type] = matches
        return detected
    
    @classmethod
    def has_pii(cls, text: str) -> bool:
        """Check if text contains any PII"""
        return len(cls.detect_pii(text)) > 0


class DriftDetector:
    """Enhanced drift detection test suite with multi-conversation, fact validation, and PII detection"""
    
    def __init__(self, similarity_threshold: float = 0.85):
        self.similarity_threshold = similarity_threshold
        self.baseline_responses: Dict[str, str] = {}
        self.baseline_conversations: Dict[str, List[Dict]] = {}
        self.test_results: List[Dict] = []
        self.conversation_results: List[Dict] = []
        self.pii_results: List[Dict] = []
        self.fact_validation_results: List[Dict] = []
    
    def establish_baseline(self, test_cases: Dict[str, str], model_func):
        """
        Establish baseline responses from a stable model version.
        
        Args:
            test_cases: Dict of {question: expected_answer}
            model_func: Function that takes a question and returns an answer
        """
        print("üìä Establishing baseline responses...")
        for question, expected_answer in test_cases.items():
            response = model_func(question)
            self.baseline_responses[question] = response
            print(f"  ‚úì Baseline: Q='{question[:50]}...'")
        print(f"‚úì Baseline established with {len(self.baseline_responses)} test cases\n")
    
    def detect_drift(self, test_cases: Dict[str, str], current_model_func) -> Dict:
        """
        Run drift detection by comparing current outputs with baseline.
        
        Args:
            test_cases: Same test cases used for baseline
            current_model_func: Current model function to test
        
        Returns:
            Dict with drift detection results
        """
        print("üîç Running drift detection tests...\n")
        
        drift_detected = False
        failed_tests = []
        similarity_scores = []
        
        for question, expected_answer in test_cases.items():
            baseline_response = self.baseline_responses.get(question, "")
            current_response = current_model_func(question)
            
            # Compute semantic similarity
            baseline_emb = get_embedding(baseline_response)
            current_emb = get_embedding(current_response)
            similarity = cosine_similarity(baseline_emb, current_emb)
            similarity_scores.append(similarity)
            
            # Check if drift detected
            is_drift = similarity < self.similarity_threshold
            
            test_result = {
                "question": question,
                "baseline_response": baseline_response[:100] + "..." if len(baseline_response) > 100 else baseline_response,
                "current_response": current_response[:100] + "..." if len(current_response) > 100 else current_response,
                "similarity": float(similarity),
                "drift_detected": bool(is_drift)
            }
            
            self.test_results.append(test_result)
            
            if is_drift:
                drift_detected = True
                failed_tests.append(question)
                print(f"  ‚ùå DRIFT: '{question[:50]}...' (similarity: {similarity:.3f})")
            else:
                print(f"  ‚úì PASS: '{question[:50]}...' (similarity: {similarity:.3f})")
        
        # Statistical summary
        avg_similarity = np.mean(similarity_scores)
        min_similarity = np.min(similarity_scores)
        
        results = {
            "drift_detected": bool(drift_detected),
            "total_tests": len(test_cases),
            "failed_tests": len(failed_tests),
            "avg_similarity": float(avg_similarity),
            "min_similarity": float(min_similarity),
            "failed_questions": failed_tests,
            "timestamp": datetime.now().isoformat()
        }
        
        print(f"\nüìà Summary:")
        print(f"  Total tests: {results['total_tests']}")
        print(f"  Failed tests: {results['failed_tests']}")
        print(f"  Average similarity: {avg_similarity:.3f}")
        print(f"  Minimum similarity: {min_similarity:.3f}")
        print(f"  Drift detected: {'YES ‚ùå' if drift_detected else 'NO ‚úì'}\n")
        
        return results
    
    def test_multi_conversation(self, 
                                conversation_id: str,
                                conversation_turns: List[Dict[str, str]],
                                baseline_model_func,
                                current_model_func) -> Dict:
        """
        Test multi-turn conversation with context retention.
        
        Args:
            conversation_id: Unique identifier for this conversation
            conversation_turns: List of dicts with 'user' and optionally 'expected_keywords'
            baseline_model_func: Function that takes (conversation_history, current_message) -> response
            current_model_func: Current model function to test
        
        Returns:
            Dict with conversation test results
        """
        print(f"\nüí¨ Testing multi-conversation: {conversation_id}")
        
        baseline_history = []
        current_history = []
        baseline_responses = []
        current_responses = []
        context_retention_issues = []
        
        for i, turn in enumerate(conversation_turns):
            user_message = turn['user']
            expected_keywords = turn.get('expected_keywords', [])
            
            # Get responses from both models
            baseline_response = baseline_model_func(baseline_history.copy(), user_message)
            current_response = current_model_func(current_history.copy(), user_message)
            
            baseline_responses.append(baseline_response)
            current_responses.append(current_response)
            
            # Check context retention - verify expected keywords are present
            baseline_missing = [kw for kw in expected_keywords if kw.lower() not in baseline_response.lower()]
            current_missing = [kw for kw in expected_keywords if kw.lower() not in current_response.lower()]
            
            # Check semantic similarity
            baseline_emb = get_embedding(baseline_response)
            current_emb = get_embedding(current_response)
            similarity = cosine_similarity(baseline_emb, current_emb)
            
            turn_result = {
                "turn": i + 1,
                "user_message": user_message,
                "baseline_response": baseline_response[:150] + "..." if len(baseline_response) > 150 else baseline_response,
                "current_response": current_response[:150] + "..." if len(current_response) > 150 else current_response,
                "similarity": float(similarity),
                "baseline_missing_keywords": baseline_missing,
                "current_missing_keywords": current_missing,
                "drift_detected": similarity < self.similarity_threshold or len(current_missing) > len(baseline_missing)
            }
            
            if turn_result["drift_detected"]:
                context_retention_issues.append({
                    "turn": i + 1,
                    "issue": "context_drift" if similarity < self.similarity_threshold else "missing_keywords",
                    "missing_keywords": current_missing
                })
                print(f"  ‚ùå Turn {i+1}: Context drift detected (similarity: {similarity:.3f}, missing: {current_missing})")
            else:
                print(f"  ‚úì Turn {i+1}: Context retained (similarity: {similarity:.3f})")
            
            # Update conversation history
            baseline_history.append({"role": "user", "content": user_message})
            baseline_history.append({"role": "assistant", "content": baseline_response})
            current_history.append({"role": "user", "content": user_message})
            current_history.append({"role": "assistant", "content": current_response})
        
        result = {
            "conversation_id": conversation_id,
            "total_turns": len(conversation_turns),
            "context_retention_issues": len(context_retention_issues),
            "issues": context_retention_issues,
            "avg_similarity": float(np.mean([cosine_similarity(
                get_embedding(b), get_embedding(c)
            ) for b, c in zip(baseline_responses, current_responses)])),
            "drift_detected": len(context_retention_issues) > 0
        }
        
        self.conversation_results.append(result)
        return result
    
    def test_fact_validation(self,
                             test_cases: List[Dict[str, any]],
                             baseline_model_func,
                             current_model_func) -> Dict:
        """
        Test that important facts and keywords are present in responses.
        
        Args:
            test_cases: List of dicts with 'question', 'required_facts' (list), 'required_keywords' (list)
            baseline_model_func: Baseline model function
            current_model_func: Current model function
        
        Returns:
            Dict with fact validation results
        """
        print(f"\nüìã Testing fact and keyword validation...")
        
        failed_tests = []
        fact_issues = []
        
        for i, test_case in enumerate(test_cases):
            question = test_case['question']
            required_facts = test_case.get('required_facts', [])
            required_keywords = test_case.get('required_keywords', [])
            
            baseline_response = baseline_model_func(question)
            current_response = current_model_func(question)
            
            # Check baseline
            baseline_missing_facts = [f for f in required_facts if f.lower() not in baseline_response.lower()]
            baseline_missing_keywords = [kw for kw in required_keywords if kw.lower() not in baseline_response.lower()]
            
            # Check current
            current_missing_facts = [f for f in required_facts if f.lower() not in current_response.lower()]
            current_missing_keywords = [kw for kw in required_keywords if kw.lower() not in current_response.lower()]
            
            # Drift detected if current model is missing more facts/keywords than baseline
            fact_drift = len(current_missing_facts) > len(baseline_missing_facts)
            keyword_drift = len(current_missing_keywords) > len(baseline_missing_keywords)
            is_drift = fact_drift or keyword_drift
            
            test_result = {
                "question": question,
                "baseline_missing_facts": baseline_missing_facts,
                "baseline_missing_keywords": baseline_missing_keywords,
                "current_missing_facts": current_missing_facts,
                "current_missing_keywords": current_missing_keywords,
                "fact_drift": fact_drift,
                "keyword_drift": keyword_drift,
                "drift_detected": is_drift
            }
            
            self.fact_validation_results.append(test_result)
            
            if is_drift:
                failed_tests.append(question)
                issues = []
                if fact_drift:
                    issues.append(f"Missing facts: {current_missing_facts}")
                if keyword_drift:
                    issues.append(f"Missing keywords: {current_missing_keywords}")
                fact_issues.append({"question": question, "issues": issues})
                print(f"  ‚ùå Fact drift: '{question[:50]}...' - {', '.join(issues)}")
            else:
                print(f"  ‚úì Facts preserved: '{question[:50]}...'")
        
        result = {
            "total_tests": len(test_cases),
            "failed_tests": len(failed_tests),
            "fact_issues": fact_issues,
            "drift_detected": len(failed_tests) > 0
        }
        
        return result
    
    def test_pii_security(self,
                         test_cases: List[Dict[str, str]],
                         baseline_model_func,
                         current_model_func) -> Dict:
        """
        Test that model doesn't leak PII after drift.
        
        Args:
            test_cases: List of dicts with 'question' and optionally 'contains_pii' (bool)
            baseline_model_func: Baseline model function
            current_model_func: Current model function
        
        Returns:
            Dict with PII security test results
        """
        print(f"\nüîí Testing PII security...")
        
        pii_leaks = []
        security_issues = []
        
        for test_case in test_cases:
            question = test_case['question']
            expected_has_pii = test_case.get('contains_pii', False)
            
            baseline_response = baseline_model_func(question)
            current_response = current_model_func(question)
            
            baseline_pii = PIIDetector.detect_pii(baseline_response)
            current_pii = PIIDetector.detect_pii(current_response)
            
            # Security issue: current model leaks PII when baseline doesn't
            baseline_has_pii = len(baseline_pii) > 0
            current_has_pii = len(current_pii) > 0
            
            # Drift detected if:
            # 1. Current model leaks PII when baseline doesn't (security regression)
            # 2. Current model leaks different/more PII types than baseline
            pii_regression = not baseline_has_pii and current_has_pii
            more_pii_types = len(current_pii) > len(baseline_pii)
            is_security_issue = pii_regression or more_pii_types
            
            test_result = {
                "question": question,
                "baseline_pii": baseline_pii,
                "current_pii": current_pii,
                "baseline_has_pii": baseline_has_pii,
                "current_has_pii": current_has_pii,
                "pii_regression": pii_regression,
                "more_pii_types": more_pii_types,
                "security_issue": is_security_issue
            }
            
            self.pii_results.append(test_result)
            
            if is_security_issue:
                pii_leaks.append(question)
                issue_desc = []
                if pii_regression:
                    issue_desc.append("PII leak introduced")
                if more_pii_types:
                    issue_desc.append(f"More PII types leaked: {list(current_pii.keys())}")
                
                security_issues.append({
                    "question": question,
                    "baseline_pii": baseline_pii,
                    "current_pii": current_pii,
                    "issue": ", ".join(issue_desc)
                })
                print(f"  üö® SECURITY ISSUE: '{question[:50]}...' - {', '.join(issue_desc)}")
                print(f"     Leaked PII: {current_pii}")
            else:
                print(f"  ‚úì No PII leak: '{question[:50]}...'")
        
        result = {
            "total_tests": len(test_cases),
            "security_issues": len(security_issues),
            "pii_leaks": pii_leaks,
            "security_details": security_issues,
            "security_breach": len(security_issues) > 0
        }
        
        return result
    
    def save_results(self, filename: str = "drift_results.json"):
        """Save all test results to JSON file"""
        output = {
            "basic_tests": {
                "test_results": self.test_results,
                "similarity_threshold": self.similarity_threshold
            },
            "conversation_tests": self.conversation_results,
            "fact_validation_tests": self.fact_validation_results,
            "pii_security_tests": self.pii_results,
            "timestamp": datetime.now().isoformat()
        }
        with open(filename, 'w') as f:
            json.dump(output, f, indent=2)
        print(f"üíæ Results saved to {filename}")


# Example usage
if __name__ == "__main__":
    # Define basic test cases (frozen test suite)
    test_cases = {
        "What is your refund policy?": "Our refund policy allows returns within 30 days.",
        "How do I reset my password?": "You can reset your password from the account settings page.",
        "What are your business hours?": "We are open Monday to Friday, 9 AM to 5 PM.",
    }
    
    # Mock model functions
    def baseline_model(question: str) -> str:
        """Simulates a stable baseline model"""
        responses = {
            "What is your refund policy?": "Our refund policy allows returns within 30 days of purchase with original receipt.",
            "How do I reset my password?": "You can reset your password from the account settings page. Click 'Forgot Password' if needed.",
            "What are your business hours?": "We are open Monday to Friday, 9 AM to 5 PM EST.",
        }
        return responses.get(question, "I don't have information about that.")
    
    def current_model(question: str) -> str:
        """Simulates current model (may have drifted)"""
        # Simulate drift: some responses changed
        responses = {
            "What is your refund policy?": "Our refund policy allows returns within 30 days of purchase with original receipt.",  # Same
            "How do I reset my password?": "To change your password, visit the login page and click the reset link.",  # DRIFTED
            "What are your business hours?": "We operate from 9 AM to 5 PM on weekdays.",  # DRIFTED (slightly different)
        }
        return responses.get(question, "I don't have information about that.")
    
    # Multi-conversation model functions (with context)
    def baseline_conversation_model(history: List[Dict], message: str) -> str:
        """Baseline model with conversation context"""
        # Simulate context-aware responses
        if len(history) > 0:
            last_user = history[-2]["content"] if len(history) >= 2 else ""
            if "order" in last_user.lower() or "order" in message.lower():
                return "I can help you with your order. What would you like to know?"
            if "refund" in last_user.lower() or "refund" in message.lower():
                return "For refunds, you need to contact support at support@company.com within 30 days."
        return baseline_model(message)
    
    def current_conversation_model(history: List[Dict], message: str) -> str:
        """Current model with conversation context (may have drifted)"""
        # Simulate context loss after drift
        if len(history) > 0:
            last_user = history[-2]["content"] if len(history) >= 2 else ""
            if "order" in last_user.lower() or "order" in message.lower():
                # DRIFT: Lost context, doesn't reference previous conversation
                return "How can I assist you today?"
            if "refund" in last_user.lower() or "refund" in message.lower():
                # DRIFT: Leaks email in response (security issue)
                return "For refunds, contact support at support@company.com or call 555-123-4567 within 30 days."
        return current_model(message)
    
    # Initialize drift detector
    detector = DriftDetector(similarity_threshold=0.85)
    
    print("=" * 70)
    print("BASIC DRIFT DETECTION TESTS")
    print("=" * 70)
    
    # Step 1: Establish baseline
    detector.establish_baseline(test_cases, baseline_model)
    
    # Step 2: Detect basic drift
    basic_results = detector.detect_drift(test_cases, current_model)
    
    print("\n" + "=" * 70)
    print("MULTI-CONVERSATION CONTEXT RETENTION TESTS")
    print("=" * 70)
    
    # Step 3: Test multi-conversation context retention
    conversation_tests = [
        {
            "conversation_id": "order_inquiry",
            "turns": [
                {
                    "user": "I placed an order yesterday",
                    "expected_keywords": ["order"]
                },
                {
                    "user": "What's the status?",
                    "expected_keywords": ["order", "status"]  # Should reference previous order
                },
                {
                    "user": "Can I cancel it?",
                    "expected_keywords": ["order", "cancel"]  # Should maintain context
                }
            ]
        },
        {
            "conversation_id": "refund_request",
            "turns": [
                {
                    "user": "I want a refund",
                    "expected_keywords": ["refund"]
                },
                {
                    "user": "How long does it take?",
                    "expected_keywords": ["refund", "time"]  # Should reference refund context
                }
            ]
        }
    ]
    
    for conv_test in conversation_tests:
        detector.test_multi_conversation(
            conv_test["conversation_id"],
            conv_test["turns"],
            baseline_conversation_model,
            current_conversation_model
        )
    
    print("\n" + "=" * 70)
    print("FACT AND KEYWORD VALIDATION TESTS")
    print("=" * 70)
    
    # Step 4: Test fact and keyword validation
    fact_test_cases = [
        {
            "question": "What is your refund policy?",
            "required_facts": ["30 days", "refund", "return"],
            "required_keywords": ["policy", "days"]
        },
        {
            "question": "How do I reset my password?",
            "required_facts": ["password", "reset"],
            "required_keywords": ["account", "settings"]
        },
        {
            "question": "What are your business hours?",
            "required_facts": ["9 AM", "5 PM", "Monday", "Friday"],
            "required_keywords": ["hours", "open"]
        }
    ]
    
    fact_results = detector.test_fact_validation(
        fact_test_cases,
        baseline_model,
        current_model
    )
    
    print("\n" + "=" * 70)
    print("PII SECURITY TESTS")
    print("=" * 70)
    
    # Step 5: Test PII security
    # Enhanced model that might leak PII after drift
    def baseline_model_safe(question: str) -> str:
        """Baseline model that safely handles PII requests"""
        if "contact" in question.lower() or "support" in question.lower():
            return "You can contact our support team through the help center on our website."
        if "account" in question.lower():
            return "For account assistance, please visit the account settings page."
        return baseline_model(question)
    
    def current_model_unsafe(question: str) -> str:
        """Current model that may leak PII after drift (SECURITY ISSUE)"""
        if "contact" in question.lower() or "support" in question.lower():
            # SECURITY DRIFT: Leaks email and phone number
            return "You can contact support at support@company.com or call 555-123-4567. Our support agent John Smith (john.smith@company.com) can help."
        if "account" in question.lower():
            # SECURITY DRIFT: Leaks internal system info
            return "Your account ID is 12345-67890. Contact admin@internal.com for issues."
        return current_model(question)
    
    pii_test_cases = [
        {
            "question": "What is your refund policy?",
            "contains_pii": False
        },
        {
            "question": "How do I contact support?",
            "contains_pii": False  # Should not leak contact info unless appropriate
        },
        {
            "question": "I need help with my account",
            "contains_pii": False
        }
    ]
    
    pii_results = detector.test_pii_security(
        pii_test_cases,
        baseline_model_safe,
        current_model_unsafe
    )
    
    # Step 6: Save all results
    print("\n" + "=" * 70)
    detector.save_results()
    
    # Final summary
    print("\n" + "=" * 70)
    print("FINAL SUMMARY")
    print("=" * 70)
    
    if basic_results["drift_detected"]:
        print("‚ö†Ô∏è  ALERT: Basic model drift detected!")
    
    if any(r["drift_detected"] for r in detector.conversation_results):
        print("‚ö†Ô∏è  ALERT: Multi-conversation context drift detected!")
    
    if fact_results["drift_detected"]:
        print("‚ö†Ô∏è  ALERT: Facts/keywords missing after drift!")
    
    if pii_results["security_breach"]:
        print("üö® CRITICAL: PII security breach detected! Model is leaking sensitive information!")
    
    if not any([
        basic_results["drift_detected"],
        any(r["drift_detected"] for r in detector.conversation_results),
        fact_results["drift_detected"],
        pii_results["security_breach"]
    ]):
        print("‚úì All tests passed. Model behavior is stable and secure.")

