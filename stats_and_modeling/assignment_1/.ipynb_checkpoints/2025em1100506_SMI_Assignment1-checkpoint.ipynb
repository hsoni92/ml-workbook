{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Statistical Modeling and Inferencing\n",
        "## Assignment 1\n",
        "\n",
        "**Name:** Himanshu Soni  \n",
        "**Roll Number:** 2025em1100506  \n",
        "**Dataset:** Wholesale Customers Dataset (Clustering Analysis)  \n",
        "**Submission Date:** December 3, 2025\n",
        "\n",
        "---\n",
        "\n",
        "**Note:** This assignment is submitted as a Jupyter Notebook. All code and analysis are contained within this document.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Executive Summary\n",
        "\n",
        "This assignment presents a comprehensive clustering analysis of the Wholesale Customers Dataset to identify distinct customer segments based on their purchasing behavior across different product categories. The analysis follows a systematic approach: data exploration and preparation, clustering model development using multiple algorithms, and interpretation of results to derive actionable business insights.\n",
        "\n",
        "The analysis identified [X] distinct customer segments through K-Means and Hierarchical clustering algorithms, each characterized by unique purchasing patterns. Key findings reveal significant differences in product preferences and purchase volumes across segments, enabling targeted marketing strategies and inventory management.\n",
        "\n",
        "The results demonstrate the practical value of unsupervised learning in customer segmentation, providing businesses with actionable insights for personalized marketing, product recommendations, and strategic decision-making. While the analysis has certain limitations related to sample size and feature availability, the identified segments offer a solid foundation for customer relationship management and business growth strategies.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset Selection\n",
        "\n",
        "**Chosen Dataset:** Dataset 2 - Wholesale Customers Dataset\n",
        "\n",
        "**Justification:**\n",
        "- The dataset is well-suited for clustering analysis with clear business applications\n",
        "- Moderate sample size (~440 observations) allows for efficient computation and clear visualization\n",
        "- Multiple numerical features enable meaningful segmentation\n",
        "- Clustering analysis provides actionable insights for customer relationship management\n",
        "- The problem domain (wholesale customer segmentation) has clear practical implications\n",
        "\n",
        "**Dataset URL:** https://archive.ics.uci.edu/ml/machine-learning-databases/00292/Wholesale%20customers%20data.csv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 0: Setup and Library Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data manipulation libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Visualization libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Clustering algorithms\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
        "\n",
        "# Preprocessing\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "# Metrics for clustering evaluation\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "\n",
        "# Dimensionality reduction\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Hierarchical clustering visualization\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "\n",
        "# Utilities\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set plotting style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "print(\"All libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 1: Data Exploration and Preparation\n",
        "\n",
        "### Step 1.1: Data Loading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset from URL\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00292/Wholesale%20customers%20data.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Display basic information\n",
        "print(\"Dataset Shape:\", df.shape)\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(df.head())\n",
        "print(\"\\nColumn Information:\")\n",
        "print(df.info())\n",
        "print(\"\\nColumn Names:\")\n",
        "print(df.columns.tolist())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Initial Observations:**\n",
        "- The dataset contains [X] rows and [X] columns\n",
        "- Features include: Channel, Region, and various product categories (Fresh, Milk, Grocery, Frozen, Detergents_Paper, Delicassen)\n",
        "- Channel and Region appear to be categorical variables\n",
        "- All other variables are numerical (representing annual spending in monetary units)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1.2: Descriptive Statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Descriptive statistics for numerical variables\n",
        "print(\"Descriptive Statistics for Numerical Variables:\")\n",
        "print(df.describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for categorical variables\n",
        "print(\"\\nCategorical Variables:\")\n",
        "print(\"\\nChannel value counts:\")\n",
        "print(df['Channel'].value_counts())\n",
        "print(\"\\nRegion value counts:\")\n",
        "print(df['Region'].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Key Statistics Interpretation:**\n",
        "- The dataset shows significant variation in spending across product categories\n",
        "- Mean values indicate which product categories are most popular\n",
        "- Standard deviations suggest high variability in customer purchasing behavior\n",
        "- The presence of Channel (likely: Hotel/Restaurant/Cafe vs Retail) and Region (likely: geographic regions) as categorical variables\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1.3: Data Quality Assessment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "missing_values = df.isnull().sum()\n",
        "missing_percentage = (missing_values / len(df)) * 100\n",
        "\n",
        "missing_df = pd.DataFrame({\n",
        "    'Missing Count': missing_values,\n",
        "    'Percentage': missing_percentage\n",
        "})\n",
        "print(\"Missing Values Analysis:\")\n",
        "print(missing_df[missing_df['Missing Count'] > 0])\n",
        "\n",
        "if missing_df['Missing Count'].sum() == 0:\n",
        "    print(\"\\n✓ No missing values found in the dataset\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize missing values (if any)\n",
        "if missing_df['Missing Count'].sum() > 0:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.heatmap(df.isnull(), yticklabels=False, cbar=True, cmap='viridis')\n",
        "    plt.title('Missing Values Heatmap')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No missing values to visualize\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for duplicate rows\n",
        "duplicate_count = df.duplicated().sum()\n",
        "print(f\"Number of duplicate rows: {duplicate_count}\")\n",
        "\n",
        "if duplicate_count > 0:\n",
        "    print(\"\\nDuplicate rows found:\")\n",
        "    print(df[df.duplicated()])\n",
        "else:\n",
        "    print(\"✓ No duplicate rows found\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Data Quality Summary:**\n",
        "- Missing values: [Document findings]\n",
        "- Duplicates: [Document findings]\n",
        "- **Handling Strategy:** [Document how issues were handled or note that no issues were found]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1.4: Outlier Detection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get numerical columns (exclude Channel and Region for now)\n",
        "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "print(\"Numerical columns:\", numerical_cols)\n",
        "\n",
        "# Create box plots for outlier detection\n",
        "n_cols = len(numerical_cols)\n",
        "n_rows = (n_cols + 2) // 3\n",
        "\n",
        "fig, axes = plt.subplots(n_rows, 3, figsize=(18, 5*n_rows))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, col in enumerate(numerical_cols):\n",
        "    df.boxplot(column=col, ax=axes[i])\n",
        "    axes[i].set_title(f'Box Plot: {col}')\n",
        "    axes[i].set_ylabel('Value')\n",
        "\n",
        "# Hide extra subplots\n",
        "for i in range(n_cols, len(axes)):\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate IQR-based outliers\n",
        "outlier_summary = []\n",
        "\n",
        "for col in numerical_cols:\n",
        "    Q1 = df[col].quantile(0.25)\n",
        "    Q3 = df[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
        "    outlier_count = len(outliers)\n",
        "    outlier_percentage = (outlier_count / len(df)) * 100\n",
        "\n",
        "    outlier_summary.append({\n",
        "        'Feature': col,\n",
        "        'Lower Bound': lower_bound,\n",
        "        'Upper Bound': upper_bound,\n",
        "        'Outlier Count': outlier_count,\n",
        "        'Outlier Percentage': outlier_percentage\n",
        "    })\n",
        "\n",
        "outlier_df = pd.DataFrame(outlier_summary)\n",
        "print(\"Outlier Detection Summary (IQR Method):\")\n",
        "print(outlier_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Z-score method for additional validation\n",
        "from scipy import stats\n",
        "\n",
        "zscore_outliers = {}\n",
        "for col in numerical_cols:\n",
        "    z_scores = np.abs(stats.zscore(df[col]))\n",
        "    outliers = df[z_scores > 3]\n",
        "    zscore_outliers[col] = len(outliers)\n",
        "\n",
        "print(\"Outlier Detection Summary (Z-score method, threshold=3):\")\n",
        "for col, count in zscore_outliers.items():\n",
        "    print(f\"{col}: {count} outliers ({count/len(df)*100:.2f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Outlier Analysis Findings:**\n",
        "- Multiple features show presence of outliers, which is expected in wholesale customer data\n",
        "- Outliers may represent high-volume customers (e.g., large retailers or restaurant chains)\n",
        "- **Handling Strategy:** We will retain outliers as they represent legitimate business cases and are important for identifying distinct customer segments. Outliers in this context are valuable for clustering analysis as they may represent unique customer groups.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1.5: Distribution Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create histograms with KDE for all numerical variables\n",
        "fig, axes = plt.subplots(n_rows, 3, figsize=(18, 5*n_rows))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, col in enumerate(numerical_cols):\n",
        "    df[col].hist(bins=30, ax=axes[i], alpha=0.7, edgecolor='black')\n",
        "    df[col].plot.density(ax=axes[i], secondary_y=False, color='red', linewidth=2)\n",
        "    axes[i].set_title(f'Distribution: {col}')\n",
        "    axes[i].set_xlabel('Value')\n",
        "    axes[i].set_ylabel('Frequency')\n",
        "    axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "# Hide extra subplots\n",
        "for i in range(n_cols, len(axes)):\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Q-Q plots for normality check\n",
        "from scipy import stats\n",
        "\n",
        "fig, axes = plt.subplots(n_rows, 3, figsize=(18, 5*n_rows))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, col in enumerate(numerical_cols):\n",
        "    stats.probplot(df[col], dist=\"norm\", plot=axes[i])\n",
        "    axes[i].set_title(f'Q-Q Plot: {col}')\n",
        "    axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "# Hide extra subplots\n",
        "for i in range(n_cols, len(axes)):\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate skewness for each numerical variable\n",
        "skewness = df[numerical_cols].skew()\n",
        "print(\"Skewness Analysis:\")\n",
        "print(skewness)\n",
        "print(\"\\nInterpretation:\")\n",
        "print(\"Values close to 0 indicate normal distribution\")\n",
        "print(\"Positive values indicate right skew, negative values indicate left skew\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Distribution Characteristics:**\n",
        "- Most variables show right-skewed distributions (positive skewness)\n",
        "- This is typical for spending data where most customers have moderate spending, but a few have very high spending\n",
        "- Q-Q plots indicate deviations from normality, which is expected for business data\n",
        "- The skewed distributions suggest that log transformation might be beneficial, but we'll evaluate this after standardization for clustering\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1.6: Correlation Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate correlation matrix for numerical variables\n",
        "correlation_matrix = df[numerical_cols].corr()\n",
        "\n",
        "# Create correlation heatmap\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm',\n",
        "            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
        "plt.title('Correlation Matrix Heatmap', fontsize=16, pad=20)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify strong correlations (absolute value > 0.5)\n",
        "strong_correlations = []\n",
        "for i in range(len(correlation_matrix.columns)):\n",
        "    for j in range(i+1, len(correlation_matrix.columns)):\n",
        "        corr_value = correlation_matrix.iloc[i, j]\n",
        "        if abs(corr_value) > 0.5:\n",
        "            strong_correlations.append({\n",
        "                'Variable 1': correlation_matrix.columns[i],\n",
        "                'Variable 2': correlation_matrix.columns[j],\n",
        "                'Correlation': corr_value\n",
        "            })\n",
        "\n",
        "if strong_correlations:\n",
        "    strong_corr_df = pd.DataFrame(strong_correlations)\n",
        "    print(\"Strong Correlations (|r| > 0.5):\")\n",
        "    print(strong_corr_df.sort_values('Correlation', key=abs, ascending=False))\n",
        "else:\n",
        "    print(\"No strong correlations found (|r| > 0.5)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Key Correlation Findings:**\n",
        "- [Document strong relationships found]\n",
        "- High correlations between certain product categories suggest customers who buy one category also tend to buy related categories\n",
        "- These relationships will be useful for understanding customer purchasing patterns in clustering analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1.7: Categorical Variable Handling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check categorical variables\n",
        "print(\"Channel unique values:\", df['Channel'].unique())\n",
        "print(\"Region unique values:\", df['Region'].unique())\n",
        "\n",
        "# For clustering, we have two options:\n",
        "# 1. Include Channel and Region as features (encoded)\n",
        "# 2. Exclude them and cluster based only on product spending\n",
        "\n",
        "# We'll create encoded versions but note that for clustering, we may choose to exclude them\n",
        "# as they represent known customer characteristics rather than purchasing behavior\n",
        "\n",
        "# Create a copy for encoding\n",
        "df_encoded = df.copy()\n",
        "\n",
        "# Label encoding for Channel and Region (preserving ordinal nature if applicable)\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le_channel = LabelEncoder()\n",
        "le_region = LabelEncoder()\n",
        "\n",
        "df_encoded['Channel_encoded'] = le_channel.fit_transform(df['Channel'])\n",
        "df_encoded['Region_encoded'] = le_region.fit_transform(df['Region'])\n",
        "\n",
        "print(\"\\nEncoded values:\")\n",
        "print(\"Channel mapping:\", dict(zip(le_channel.classes_, range(len(le_channel.classes_)))))\n",
        "print(\"Region mapping:\", dict(zip(le_region.classes_, range(len(le_region.classes_)))))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Encoding Justification:**\n",
        "- Used Label Encoding for Channel and Region as they are ordinal/categorical variables\n",
        "- For clustering analysis, we will primarily focus on product spending patterns (numerical features)\n",
        "- Channel and Region can be used for validation/comparison but may not be included in the main clustering features\n",
        "- This approach allows us to discover natural customer segments based on purchasing behavior rather than pre-defined categories\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1.8: Feature Transformations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Given the right-skewed distributions, we could apply log transformation\n",
        "# However, for clustering, standardization is more critical\n",
        "# We'll apply log transformation to see the effect, but final decision will be made during clustering phase\n",
        "\n",
        "# Create log-transformed version for comparison\n",
        "df_log = df[numerical_cols].copy()\n",
        "for col in numerical_cols:\n",
        "    df_log[f'{col}_log'] = np.log1p(df[col])  # log1p to handle zeros\n",
        "\n",
        "print(\"Log transformation applied (using log1p to handle zeros)\")\n",
        "print(\"Original vs Log-transformed skewness comparison:\")\n",
        "\n",
        "original_skew = df[numerical_cols].skew()\n",
        "log_skew = df_log[[f'{col}_log' for col in numerical_cols]].skew()\n",
        "\n",
        "comparison = pd.DataFrame({\n",
        "    'Original Skewness': original_skew,\n",
        "    'Log-transformed Skewness': log_skew\n",
        "})\n",
        "print(comparison)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Transformation Decision:**\n",
        "- Log transformation reduces skewness significantly\n",
        "- However, for clustering with distance-based algorithms, standardization is more important than normalization of distributions\n",
        "- We will use StandardScaler in the clustering phase, which will handle the scale differences\n",
        "- Log transformation may be applied if it improves clustering quality, but we'll start with standardized original features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1.9: Final Data Preparation Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== DATA PREPARATION SUMMARY ===\\n\")\n",
        "print(f\"Original Dataset Shape: {df.shape}\")\n",
        "print(f\"\\nData Quality Issues Found:\")\n",
        "print(f\"  - Missing Values: {df.isnull().sum().sum()} (None found)\")\n",
        "print(f\"  - Duplicate Rows: {df.duplicated().sum()} (None found)\")\n",
        "print(f\"\\nFeatures:\")\n",
        "print(f\"  - Numerical Features: {len(numerical_cols)}\")\n",
        "print(f\"    {numerical_cols}\")\n",
        "print(f\"  - Categorical Features: Channel, Region\")\n",
        "print(f\"\\nPreprocessing Steps Completed:\")\n",
        "print(\"  1. ✓ Data loaded and basic information extracted\")\n",
        "print(\"  2. ✓ Descriptive statistics generated\")\n",
        "print(\"  3. ✓ Missing values and duplicates checked\")\n",
        "print(\"  4. ✓ Outliers identified (retained for clustering)\")\n",
        "print(\"  5. ✓ Distribution analysis completed\")\n",
        "print(\"  6. ✓ Correlation analysis completed\")\n",
        "print(\"  7. ✓ Categorical variables encoded\")\n",
        "print(\"  8. ✓ Feature transformation options evaluated\")\n",
        "print(f\"\\nFinal Dataset Ready for Clustering:\")\n",
        "print(f\"  - Shape: {df.shape}\")\n",
        "print(f\"  - All features identified and prepared\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 2: Model Development and Validation\n",
        "\n",
        "### Step 2.1: Data Preprocessing for Clustering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For clustering, we'll focus on product spending patterns\n",
        "# Exclude Channel and Region from clustering features (we can use them for validation later)\n",
        "features_for_clustering = numerical_cols.copy()\n",
        "print(\"Features selected for clustering:\", features_for_clustering)\n",
        "\n",
        "# Extract features\n",
        "X = df[features_for_clustering].copy()\n",
        "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
        "print(\"First few rows:\")\n",
        "print(X.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standardization is critical for clustering algorithms that use distance metrics\n",
        "# We'll use StandardScaler (z-score normalization)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_scaled_df = pd.DataFrame(X_scaled, columns=features_for_clustering, index=X.index)\n",
        "\n",
        "print(\"Data standardized using StandardScaler\")\n",
        "print(\"\\nOriginal data statistics:\")\n",
        "print(X.describe())\n",
        "print(\"\\nStandardized data statistics:\")\n",
        "print(X_scaled_df.describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Scaler Choice Justification:**\n",
        "- **StandardScaler (Z-score normalization)** chosen over MinMaxScaler\n",
        "- StandardScaler centers data around mean=0 and scales to std=1, which is ideal for distance-based clustering algorithms\n",
        "- Preserves the relative relationships between features while removing scale differences\n",
        "- Works well with K-Means and Hierarchical clustering algorithms\n",
        "- StandardScaler is less sensitive to outliers than MinMaxScaler, which is important given the presence of high-volume customers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2.2: Determine Optimal Number of Clusters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Method 1: Elbow Method\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate WCSS (Within-Cluster Sum of Squares) for k=1 to k=10\n",
        "wcss = []\n",
        "k_range = range(1, 11)\n",
        "\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans.fit(X_scaled)\n",
        "    wcss.append(kmeans.inertia_)\n",
        "\n",
        "# Plot the Elbow curve\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(k_range, wcss, marker='o', linestyle='--', linewidth=2, markersize=8)\n",
        "plt.xlabel('Number of Clusters (k)', fontsize=12)\n",
        "plt.ylabel('WCSS (Within-Cluster Sum of Squares)', fontsize=12)\n",
        "plt.title('Elbow Method for Optimal k', fontsize=14, fontweight='bold')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.xticks(k_range)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Display WCSS values\n",
        "wcss_df = pd.DataFrame({'k': k_range, 'WCSS': wcss})\n",
        "print(\"WCSS Values:\")\n",
        "print(wcss_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Method 2: Silhouette Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate silhouette scores for k=2 to k=10\n",
        "silhouette_scores = []\n",
        "k_range_sil = range(2, 11)\n",
        "\n",
        "for k in k_range_sil:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    cluster_labels = kmeans.fit_predict(X_scaled)\n",
        "    silhouette_avg = silhouette_score(X_scaled, cluster_labels)\n",
        "    silhouette_scores.append(silhouette_avg)\n",
        "\n",
        "# Plot silhouette scores\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(k_range_sil, silhouette_scores, marker='o', linestyle='--', linewidth=2, markersize=8, color='green')\n",
        "plt.xlabel('Number of Clusters (k)', fontsize=12)\n",
        "plt.ylabel('Average Silhouette Score', fontsize=12)\n",
        "plt.title('Silhouette Analysis for Optimal k', fontsize=14, fontweight='bold')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.xticks(k_range_sil)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Display silhouette scores\n",
        "silhouette_df = pd.DataFrame({'k': k_range_sil, 'Silhouette Score': silhouette_scores})\n",
        "print(\"Silhouette Scores:\")\n",
        "print(silhouette_df)\n",
        "print(f\"\\nOptimal k based on highest silhouette score: k={k_range_sil[np.argmax(silhouette_scores)]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare both methods\n",
        "comparison_df = pd.DataFrame({\n",
        "    'k': list(k_range_sil),\n",
        "    'WCSS': wcss[1:],  # Skip k=1\n",
        "    'Silhouette Score': silhouette_scores\n",
        "})\n",
        "\n",
        "print(\"Comparison of Elbow Method and Silhouette Analysis:\")\n",
        "print(comparison_df)\n",
        "\n",
        "# Determine optimal k\n",
        "optimal_k_silhouette = k_range_sil[np.argmax(silhouette_scores)]\n",
        "print(f\"\\nRecommended optimal k based on Silhouette Analysis: {optimal_k_silhouette}\")\n",
        "print(f\"Silhouette Score at k={optimal_k_silhouette}: {max(silhouette_scores):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Optimal k Selection:**\n",
        "- **Elbow Method:** The elbow point appears at k=[X] (visual inspection of the plot)\n",
        "- **Silhouette Analysis:** Optimal k = [X] with highest silhouette score of [X]\n",
        "- **Final Decision:** We will use k=[X] for clustering as it provides the best balance between cluster quality and interpretability\n",
        "- This choice is justified by [explain reasoning based on results]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2.3: Apply Clustering Algorithms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use optimal k determined from silhouette analysis (calculated in previous cells)\n",
        "# If optimal_k_silhouette was calculated, use it; otherwise default to a reasonable value\n",
        "try:\n",
        "    optimal_k = optimal_k_silhouette\n",
        "    print(f\"Using optimal k={optimal_k} from silhouette analysis\")\n",
        "except NameError:\n",
        "    # Fallback: use the k with highest silhouette score if variable not found\n",
        "    # This should not happen if cells are run in order, but provides safety\n",
        "    optimal_k = 3\n",
        "    print(f\"Using default k={optimal_k}. Please ensure silhouette analysis cell was executed.\")\n",
        "\n",
        "print(f\"Applying clustering algorithms with k={optimal_k}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Algorithm 1: K-Means Clustering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply K-Means clustering\n",
        "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
        "kmeans_labels = kmeans.fit_predict(X_scaled)\n",
        "kmeans_centroids = kmeans.cluster_centers_\n",
        "\n",
        "# Add cluster labels to original dataframe\n",
        "df['KMeans_Cluster'] = kmeans_labels\n",
        "\n",
        "print(f\"K-Means clustering completed with k={optimal_k}\")\n",
        "print(f\"Cluster distribution:\")\n",
        "print(df['KMeans_Cluster'].value_counts().sort_index())\n",
        "\n",
        "# Display cluster centroids (in scaled space)\n",
        "centroids_df = pd.DataFrame(kmeans_centroids, columns=features_for_clustering)\n",
        "print(\"\\nCluster Centroids (in scaled space):\")\n",
        "print(centroids_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Algorithm 2: Hierarchical Clustering (Agglomerative)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create dendrogram for hierarchical clustering visualization\n",
        "# Using a sample for faster computation (full dataset can be slow)\n",
        "sample_size = min(50, len(X_scaled))\n",
        "sample_indices = np.random.choice(len(X_scaled), sample_size, replace=False)\n",
        "X_sample = X_scaled[sample_indices]\n",
        "\n",
        "# Create linkage matrix\n",
        "linkage_matrix = linkage(X_sample, method='ward')\n",
        "\n",
        "# Plot dendrogram\n",
        "plt.figure(figsize=(15, 8))\n",
        "dendrogram(linkage_matrix, truncate_mode='level', p=5)\n",
        "plt.title('Hierarchical Clustering Dendrogram (Sample)', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Sample Index or (Cluster Size)', fontsize=12)\n",
        "plt.ylabel('Distance', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply Agglomerative Clustering with optimal k\n",
        "agg_clustering = AgglomerativeClustering(n_clusters=optimal_k, linkage='ward')\n",
        "agg_labels = agg_clustering.fit_predict(X_scaled)\n",
        "\n",
        "# Add cluster labels to dataframe\n",
        "df['Agglomerative_Cluster'] = agg_labels\n",
        "\n",
        "print(f\"Agglomerative Clustering completed with k={optimal_k}\")\n",
        "print(f\"Cluster distribution:\")\n",
        "print(df['Agglomerative_Cluster'].value_counts().sort_index())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Algorithm 3: DBSCAN (Optional - for comparison)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DBSCAN doesn't require specifying number of clusters\n",
        "# We need to tune eps and min_samples parameters\n",
        "# Using a heuristic approach: eps = 0.5, min_samples = 5\n",
        "\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "dbscan_labels = dbscan.fit_predict(X_scaled)\n",
        "\n",
        "# Add cluster labels\n",
        "df['DBSCAN_Cluster'] = dbscan_labels\n",
        "\n",
        "n_clusters_dbscan = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n",
        "n_noise = list(dbscan_labels).count(-1)\n",
        "\n",
        "print(f\"DBSCAN clustering completed\")\n",
        "print(f\"Number of clusters found: {n_clusters_dbscan}\")\n",
        "print(f\"Number of noise points: {n_noise}\")\n",
        "print(f\"Cluster distribution:\")\n",
        "print(pd.Series(dbscan_labels).value_counts().sort_index())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2.4: Evaluate Clustering Quality\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate evaluation metrics for each clustering algorithm\n",
        "metrics_results = []\n",
        "\n",
        "# K-Means metrics\n",
        "kmeans_silhouette = silhouette_score(X_scaled, kmeans_labels)\n",
        "kmeans_db = davies_bouldin_score(X_scaled, kmeans_labels)\n",
        "kmeans_ch = calinski_harabasz_score(X_scaled, kmeans_labels)\n",
        "\n",
        "metrics_results.append({\n",
        "    'Algorithm': 'K-Means',\n",
        "    'Silhouette Score': kmeans_silhouette,\n",
        "    'Davies-Bouldin Index': kmeans_db,\n",
        "    'Calinski-Harabasz Index': kmeans_ch\n",
        "})\n",
        "\n",
        "# Agglomerative Clustering metrics\n",
        "agg_silhouette = silhouette_score(X_scaled, agg_labels)\n",
        "agg_db = davies_bouldin_score(X_scaled, agg_labels)\n",
        "agg_ch = calinski_harabasz_score(X_scaled, agg_labels)\n",
        "\n",
        "metrics_results.append({\n",
        "    'Algorithm': 'Agglomerative',\n",
        "    'Silhouette Score': agg_silhouette,\n",
        "    'Davies-Bouldin Index': agg_db,\n",
        "    'Calinski-Harabasz Index': agg_ch\n",
        "})\n",
        "\n",
        "# DBSCAN metrics (only if clusters found)\n",
        "if n_clusters_dbscan > 1:\n",
        "    dbscan_silhouette = silhouette_score(X_scaled, dbscan_labels)\n",
        "    dbscan_db = davies_bouldin_score(X_scaled, dbscan_labels)\n",
        "    dbscan_ch = calinski_harabasz_score(X_scaled, dbscan_labels)\n",
        "\n",
        "    metrics_results.append({\n",
        "        'Algorithm': 'DBSCAN',\n",
        "        'Silhouette Score': dbscan_silhouette,\n",
        "        'Davies-Bouldin Index': dbscan_db,\n",
        "        'Calinski-Harabasz Index': dbscan_ch\n",
        "    })\n",
        "\n",
        "# Create comparison table\n",
        "metrics_df = pd.DataFrame(metrics_results)\n",
        "print(\"Clustering Quality Metrics Comparison:\")\n",
        "print(metrics_df.round(4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Quality Metrics Interpretation:**\n",
        "- **Silhouette Score:** Higher is better (range: -1 to 1). Measures how similar an object is to its own cluster vs other clusters.\n",
        "- **Davies-Bouldin Index:** Lower is better. Measures average similarity ratio of each cluster with its most similar cluster.\n",
        "- **Calinski-Harabasz Index:** Higher is better. Ratio of between-cluster dispersion to within-cluster dispersion.\n",
        "\n",
        "**Assessment:** [Document which algorithm performs best based on metrics]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2.5: Visualize Clustering Results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### PCA Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply PCA for 2D visualization\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Calculate variance explained\n",
        "variance_explained = pca.explained_variance_ratio_\n",
        "cumulative_variance = np.cumsum(variance_explained)\n",
        "\n",
        "print(f\"Variance explained by PC1: {variance_explained[0]:.4f} ({variance_explained[0]*100:.2f}%)\")\n",
        "print(f\"Variance explained by PC2: {variance_explained[1]:.4f} ({variance_explained[1]*100:.2f}%)\")\n",
        "print(f\"Total variance explained: {cumulative_variance[1]:.4f} ({cumulative_variance[1]*100:.2f}%)\")\n",
        "\n",
        "# Visualize K-Means clusters in 2D\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# K-Means visualization\n",
        "scatter1 = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], c=kmeans_labels, cmap='viridis', s=50, alpha=0.6)\n",
        "axes[0].set_xlabel(f'PC1 ({variance_explained[0]*100:.2f}% variance)', fontsize=12)\n",
        "axes[0].set_ylabel(f'PC2 ({variance_explained[1]*100:.2f}% variance)', fontsize=12)\n",
        "axes[0].set_title('K-Means Clustering (PCA Visualization)', fontsize=14, fontweight='bold')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "plt.colorbar(scatter1, ax=axes[0], label='Cluster')\n",
        "\n",
        "# Agglomerative visualization\n",
        "scatter2 = axes[1].scatter(X_pca[:, 0], X_pca[:, 1], c=agg_labels, cmap='plasma', s=50, alpha=0.6)\n",
        "axes[1].set_xlabel(f'PC1 ({variance_explained[0]*100:.2f}% variance)', fontsize=12)\n",
        "axes[1].set_ylabel(f'PC2 ({variance_explained[1]*100:.2f}% variance)', fontsize=12)\n",
        "axes[1].set_title('Agglomerative Clustering (PCA Visualization)', fontsize=14, fontweight='bold')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "plt.colorbar(scatter2, ax=axes[1], label='Cluster')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Cluster Characteristics Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate mean values for each cluster (K-Means)\n",
        "cluster_means_kmeans = df.groupby('KMeans_Cluster')[features_for_clustering].mean()\n",
        "print(\"K-Means Cluster Characteristics (Mean Values):\")\n",
        "print(cluster_means_kmeans.round(2))\n",
        "\n",
        "# Create heatmap for cluster characteristics\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.heatmap(cluster_means_kmeans.T, annot=True, fmt='.0f', cmap='YlOrRd',\n",
        "            cbar_kws={'label': 'Mean Spending'})\n",
        "plt.title('K-Means Cluster Characteristics Heatmap', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Cluster', fontsize=12)\n",
        "plt.ylabel('Product Category', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate mean values for Agglomerative clusters\n",
        "cluster_means_agg = df.groupby('Agglomerative_Cluster')[features_for_clustering].mean()\n",
        "print(\"\\nAgglomerative Cluster Characteristics (Mean Values):\")\n",
        "print(cluster_means_agg.round(2))\n",
        "\n",
        "# Create heatmap\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.heatmap(cluster_means_agg.T, annot=True, fmt='.0f', cmap='YlGnBu',\n",
        "            cbar_kws={'label': 'Mean Spending'})\n",
        "plt.title('Agglomerative Cluster Characteristics Heatmap', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Cluster', fontsize=12)\n",
        "plt.ylabel('Product Category', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Pairwise Feature Plots\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create pairwise scatter plots for key feature pairs\n",
        "# Select pairs with high correlation for better visualization\n",
        "key_pairs = [\n",
        "    ('Grocery', 'Milk'),\n",
        "    ('Grocery', 'Detergents_Paper'),\n",
        "    ('Fresh', 'Frozen')\n",
        "]\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "for idx, (feat1, feat2) in enumerate(key_pairs):\n",
        "    scatter = axes[idx].scatter(df[feat1], df[feat2], c=kmeans_labels,\n",
        "                               cmap='viridis', s=50, alpha=0.6)\n",
        "    axes[idx].set_xlabel(feat1, fontsize=11)\n",
        "    axes[idx].set_ylabel(feat2, fontsize=11)\n",
        "    axes[idx].set_title(f'{feat1} vs {feat2}', fontsize=12, fontweight='bold')\n",
        "    axes[idx].grid(True, alpha=0.3)\n",
        "    plt.colorbar(scatter, ax=axes[idx], label='Cluster')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2.6: Cluster Interpretation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed cluster profiling for K-Means (best performing algorithm)\n",
        "print(\"=== K-MEANS CLUSTER PROFILES ===\\n\")\n",
        "\n",
        "for cluster_id in sorted(df['KMeans_Cluster'].unique()):\n",
        "    cluster_data = df[df['KMeans_Cluster'] == cluster_id]\n",
        "    cluster_size = len(cluster_data)\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"CLUSTER {cluster_id} (Size: {cluster_size} customers, {cluster_size/len(df)*100:.1f}%)\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Mean values\n",
        "    print(\"\\nAverage Spending (Mean):\")\n",
        "    means = cluster_data[features_for_clustering].mean()\n",
        "    for feature, value in means.items():\n",
        "        print(f\"  {feature:20s}: {value:10.2f}\")\n",
        "\n",
        "    # Median values\n",
        "    print(\"\\nMedian Spending:\")\n",
        "    medians = cluster_data[features_for_clustering].median()\n",
        "    for feature, value in medians.items():\n",
        "        print(f\"  {feature:20s}: {value:10.2f}\")\n",
        "\n",
        "    # Channel and Region distribution\n",
        "    if 'Channel' in df.columns:\n",
        "        print(f\"\\nChannel Distribution:\")\n",
        "        print(cluster_data['Channel'].value_counts())\n",
        "    if 'Region' in df.columns:\n",
        "        print(f\"\\nRegion Distribution:\")\n",
        "        print(cluster_data['Region'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create summary table of cluster profiles\n",
        "cluster_summary = []\n",
        "\n",
        "for cluster_id in sorted(df['KMeans_Cluster'].unique()):\n",
        "    cluster_data = df[df['KMeans_Cluster'] == cluster_id]\n",
        "\n",
        "    # Calculate characteristics\n",
        "    profile = {\n",
        "        'Cluster': cluster_id,\n",
        "        'Size': len(cluster_data),\n",
        "        'Percentage': len(cluster_data)/len(df)*100\n",
        "    }\n",
        "\n",
        "    # Add mean values for each feature\n",
        "    for feature in features_for_clustering:\n",
        "        profile[f'{feature}_Mean'] = cluster_data[feature].mean()\n",
        "\n",
        "    # Identify dominant characteristics\n",
        "    means = cluster_data[features_for_clustering].mean()\n",
        "    top_features = means.nlargest(2).index.tolist()\n",
        "    profile['Top_Features'] = ', '.join(top_features)\n",
        "\n",
        "    cluster_summary.append(profile)\n",
        "\n",
        "cluster_summary_df = pd.DataFrame(cluster_summary)\n",
        "print(\"Cluster Summary Table:\")\n",
        "print(cluster_summary_df[['Cluster', 'Size', 'Percentage', 'Top_Features']])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Cluster Labels and Interpretations:**\n",
        "\n",
        "Based on the cluster characteristics analysis, we can label each cluster:\n",
        "\n",
        "- **Cluster 0:** [Label based on characteristics, e.g., \"High-Volume Retailers\"]\n",
        "  - Characteristics: [Describe key features]\n",
        "  \n",
        "- **Cluster 1:** [Label, e.g., \"Small Restaurants/Cafes\"]\n",
        "  - Characteristics: [Describe key features]\n",
        "  \n",
        "- **Cluster 2:** [Label, e.g., \"Medium-Sized Grocery Stores\"]\n",
        "  - Characteristics: [Describe key features]\n",
        "\n",
        "[Add more clusters if k > 3]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2.7: Select Best Clustering Solution\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Best Clustering Algorithm Selection:**\n",
        "\n",
        "After comparing K-Means, Agglomerative Clustering, and DBSCAN based on:\n",
        "\n",
        "1. **Quality Metrics:**\n",
        "   - Silhouette Score: [Document which is highest]\n",
        "   - Davies-Bouldin Index: [Document which is lowest]\n",
        "   - Calinski-Harabasz Index: [Document which is highest]\n",
        "\n",
        "2. **Interpretability:**\n",
        "   - [Document which algorithm produces more interpretable clusters]\n",
        "\n",
        "3. **Stability:**\n",
        "   - [Document which algorithm is more stable/reproducible]\n",
        "\n",
        "**Final Selection:** [K-Means / Agglomerative / DBSCAN]\n",
        "\n",
        "**Justification:** [Provide clear reasoning for the selection]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 3: Interpretation and Insights\n",
        "\n",
        "### Step 3.1: Key Findings Summary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The clustering analysis of the Wholesale Customers Dataset revealed [X] distinct customer segments, each characterized by unique purchasing patterns across product categories. The analysis successfully identified natural groupings in the customer base, with clear differences in spending behavior, product preferences, and purchase volumes.\n",
        "\n",
        "Key insights include the identification of [describe main segments], where customers show distinct preferences for certain product combinations. For instance, [provide specific example of a pattern found]. The clustering solution achieved a silhouette score of [X], indicating [good/moderate] cluster separation and cohesion.\n",
        "\n",
        "The analysis demonstrates that customer segmentation based on purchasing behavior provides actionable insights for business strategy. The identified segments can be leveraged for targeted marketing, personalized product recommendations, and optimized inventory management, ultimately leading to improved customer satisfaction and business profitability.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3.2: Practical Implications\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**What do customer segments represent?**\n",
        "\n",
        "The identified clusters represent distinct customer types in the wholesale market:\n",
        "- [Segment 1 name]: Represents [description] with characteristics such as [key features]\n",
        "- [Segment 2 name]: Represents [description] with characteristics such as [key features]\n",
        "- [Segment 3 name]: Represents [description] with characteristics such as [key features]\n",
        "\n",
        "**How can businesses use these segments?**\n",
        "\n",
        "1. **Targeted Marketing:** Develop segment-specific marketing campaigns tailored to each customer group's preferences and purchasing behavior.\n",
        "\n",
        "2. **Product Recommendations:** Use cluster characteristics to recommend complementary products. For example, customers in [segment] who buy [product A] are likely to be interested in [product B].\n",
        "\n",
        "3. **Inventory Management:** Optimize stock levels based on segment demand patterns. High-volume segments may require different inventory strategies than low-volume segments.\n",
        "\n",
        "4. **Pricing Strategies:** Implement dynamic pricing based on segment characteristics. High-volume segments might receive bulk discounts, while specialized segments might pay premium prices.\n",
        "\n",
        "5. **Customer Relationship Management:** Personalize interactions based on segment membership, improving customer satisfaction and retention.\n",
        "\n",
        "**Marketing Strategies for Each Segment:**\n",
        "\n",
        "- **[Segment 1]:** [Specific marketing strategy]\n",
        "- **[Segment 2]:** [Specific marketing strategy]\n",
        "- **[Segment 3]:** [Specific marketing strategy]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3.3: Limitations Discussion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Dataset Limitations:**\n",
        "\n",
        "1. **Sample Size:** With approximately 440 observations, the dataset is relatively small. A larger sample would provide more robust cluster definitions and better generalization.\n",
        "\n",
        "2. **Feature Availability:** The dataset only includes annual spending across product categories. Additional features such as customer demographics, geographic location details, purchase frequency, seasonal patterns, or customer lifetime value would enrich the analysis.\n",
        "\n",
        "3. **Temporal Information:** The dataset lacks temporal information (e.g., purchase dates, trends over time). This prevents analysis of seasonal patterns, trends, or customer behavior evolution.\n",
        "\n",
        "4. **Contextual Information:** Limited information about customer types (Channel and Region are categorical but lack detailed context) restricts deeper understanding of segment characteristics.\n",
        "\n",
        "**Methodology Limitations:**\n",
        "\n",
        "1. **Distance-Based Assumptions:** K-Means and Hierarchical clustering assume spherical clusters and may not capture complex, non-linear relationships in the data.\n",
        "\n",
        "2. **Feature Scaling Dependency:** The clustering results are sensitive to the scaling method chosen. Different scalers (StandardScaler vs MinMaxScaler) might yield different cluster assignments.\n",
        "\n",
        "3. **Optimal k Selection:** The choice of optimal k involves some subjectivity, especially when elbow and silhouette methods suggest different values.\n",
        "\n",
        "4. **Outlier Handling:** High-volume customers (outliers) are retained but may influence cluster centroids, potentially skewing cluster definitions.\n",
        "\n",
        "**Assumptions Made:**\n",
        "\n",
        "1. All features are equally important for clustering (no feature weighting applied)\n",
        "2. StandardScaler is appropriate for all features\n",
        "3. Euclidean distance is suitable for measuring customer similarity\n",
        "4. The optimal number of clusters remains stable over time\n",
        "\n",
        "**Potential Biases:**\n",
        "\n",
        "1. **Temporal Bias:** If data was collected during a specific time period, it may not represent year-round customer behavior.\n",
        "2. **Selection Bias:** The dataset may not represent the entire customer population if certain customer types are over/under-represented.\n",
        "3. **Measurement Bias:** Annual spending aggregates may mask important short-term purchasing patterns.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3.4: Recommendations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Business Strategies Based on Findings:**\n",
        "\n",
        "1. **Segment-Specific Product Bundles:** Create product bundles tailored to each segment's purchasing patterns. For example, [specific recommendation based on cluster analysis].\n",
        "\n",
        "2. **Dynamic Inventory Allocation:** Allocate inventory based on segment demand. High-volume segments may require dedicated supply chains or priority restocking.\n",
        "\n",
        "3. **Customer Acquisition:** Focus marketing efforts on acquiring customers similar to high-value segments. Use cluster characteristics to identify potential high-value customers.\n",
        "\n",
        "4. **Retention Strategies:** Develop segment-specific retention programs. For instance, [specific strategy for a segment].\n",
        "\n",
        "**Customer Targeting Approaches:**\n",
        "\n",
        "1. **New Customer Classification:** When a new customer makes initial purchases, quickly classify them into a segment to provide personalized service from the start.\n",
        "\n",
        "2. **Cross-Selling Opportunities:** Identify cross-selling opportunities based on segment patterns. Customers in [segment X] who buy [product A] are likely candidates for [product B].\n",
        "\n",
        "3. **Upselling Strategies:** Target customers in lower-volume segments with upselling campaigns based on what similar customers in higher-volume segments purchase.\n",
        "\n",
        "**Additional Data Recommendations:**\n",
        "\n",
        "1. **Temporal Data:** Collect purchase timestamps to analyze seasonal patterns, purchase frequency, and customer behavior evolution over time.\n",
        "\n",
        "2. **Customer Demographics:** Include customer size (number of employees, store size), industry type, geographic location details, and business model information.\n",
        "\n",
        "3. **Behavioral Data:** Track purchase frequency, average order value, customer lifetime value, and engagement metrics (website visits, inquiries).\n",
        "\n",
        "4. **External Factors:** Include economic indicators, seasonal factors, or market trends that might influence purchasing behavior.\n",
        "\n",
        "**Future Analysis Directions:**\n",
        "\n",
        "1. **Time Series Clustering:** Analyze how customer segments evolve over time and identify customers transitioning between segments.\n",
        "\n",
        "2. **Hierarchical Segmentation:** Create sub-segments within main clusters for more granular targeting.\n",
        "\n",
        "3. **Predictive Modeling:** Build models to predict which segment a new customer will belong to based on initial purchases.\n",
        "\n",
        "4. **Association Rule Mining:** Discover product association rules within each segment to identify cross-selling opportunities.\n",
        "\n",
        "5. **Anomaly Detection:** Identify unusual purchasing patterns that might indicate fraud, errors, or emerging customer needs.\n",
        "\n",
        "**Model Improvements:**\n",
        "\n",
        "1. **Feature Engineering:** Create derived features such as spending ratios (e.g., Grocery/Fresh ratio), total spending, or category diversity scores.\n",
        "\n",
        "2. **Alternative Algorithms:** Explore density-based clustering (DBSCAN with optimized parameters), Gaussian Mixture Models, or spectral clustering for potentially better results.\n",
        "\n",
        "3. **Validation Methods:** Implement cross-validation or bootstrap methods to assess cluster stability and reliability.\n",
        "\n",
        "4. **Dimensionality Reduction:** Apply PCA or t-SNE before clustering to reduce noise and improve cluster quality, especially if more features are added.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "This analysis successfully identified distinct customer segments in the Wholesale Customers Dataset using clustering techniques. The findings provide actionable insights for business strategy, marketing, and customer relationship management. While the analysis has limitations, it establishes a solid foundation for data-driven decision-making in customer segmentation.\n",
        "\n",
        "**Key Takeaways:**\n",
        "- [X] distinct customer segments identified\n",
        "- Clear purchasing patterns and preferences discovered\n",
        "- Actionable business strategies recommended\n",
        "- Framework established for future analysis and model improvements\n",
        "\n",
        "---\n",
        "\n",
        "**End of Assignment**\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
